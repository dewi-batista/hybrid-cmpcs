{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3e5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "repo_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027ac459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils.bins_samplers import GaussianQMCSampler\n",
    "from models.cm import ContinuousMixture\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import load_debd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpus = None if device == 'cpu' else 1\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632e372",
   "metadata": {},
   "source": [
    "### Specify the datasets to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6a9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tretail', 'dna']\n"
     ]
    }
   ],
   "source": [
    "DEBD_DATASETS = [\n",
    "    # 'nltcs',\n",
    "    # 'msnbc',\n",
    "    # 'kdd',\n",
    "    # 'plants',\n",
    "    # 'baudio',\n",
    "    # 'jester',\n",
    "    # 'bnetflix',\n",
    "    # 'accidents',\n",
    "    'tretail',\n",
    "    # 'pumsb_star',\n",
    "    'dna',\n",
    "    # 'kosarek',\n",
    "    # 'msweb',\n",
    "    # 'book',\n",
    "    # 'tmovie',\n",
    "    # 'cwebkb',\n",
    "    # 'cr52',\n",
    "    # 'c20ng',\n",
    "    # 'bbc',\n",
    "    # 'ad',\n",
    "]\n",
    "print(DEBD_DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb4965",
   "metadata": {},
   "source": [
    "### Number of integration points (bins) to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91cbd151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 256, 512, 1024, 2048, 4096, 8192]\n"
     ]
    }
   ],
   "source": [
    "n_bins_list = [2**7, 2**8, 2**9, 2**10, 2**11, 2**12, 2**13]\n",
    "# n_bins_list = [2**13]\n",
    "print(n_bins_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289a9ac",
   "metadata": {},
   "source": [
    "### Set clt to False (True) for CM of factorisations (of CLTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216b994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/s3313093/cm-tpm-main/logs/debd/cm_fact/\n"
     ]
    }
   ],
   "source": [
    "clt = False\n",
    "log_dir = repo_dir + ('/logs/debd/cm_clt/' if clt else '/logs/debd/cm_fact/')\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258a620",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939fed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lls_dict(lls_dict):\n",
    "    for n_bins in lls_dict.keys():\n",
    "        avg_lls_per_run = [np.mean(ll) for ll in lls_dict[n_bins]]\n",
    "        avg_ll = np.mean(avg_lls_per_run)\n",
    "        std_ll = np.std(avg_lls_per_run)\n",
    "        print('Evaluating using ' + str(n_bins) + ' bins..')\n",
    "        print('AVG LL: %f ' % avg_ll + ' STD LL: %f ' % std_ll)\n",
    "        print('Latex string: %.2f$\\\\pm$%.2f' % (avg_ll, std_ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03dcfa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating tretail..\n",
      "\n",
      " --- BMV on TEST ---\n",
      "Evaluating using 128 bins..\n",
      "AVG LL: -10.958385  STD LL: 0.029157 \n",
      "Latex string: -10.96$\\pm$0.03\n",
      "Evaluating using 256 bins..\n",
      "AVG LL: -10.899335  STD LL: 0.006448 \n",
      "Latex string: -10.90$\\pm$0.01\n",
      "Evaluating using 512 bins..\n",
      "AVG LL: -10.861258  STD LL: 0.006889 \n",
      "Latex string: -10.86$\\pm$0.01\n",
      "Evaluating using 1024 bins..\n",
      "AVG LL: -10.850134  STD LL: 0.004201 \n",
      "Latex string: -10.85$\\pm$0.00\n",
      "Evaluating using 2048 bins..\n",
      "AVG LL: -10.847236  STD LL: 0.004806 \n",
      "Latex string: -10.85$\\pm$0.00\n",
      "Evaluating using 4096 bins..\n",
      "AVG LL: -10.846115  STD LL: 0.005021 \n",
      "Latex string: -10.85$\\pm$0.01\n",
      "Evaluating using 8192 bins..\n",
      "AVG LL: -10.845361  STD LL: 0.005040 \n",
      "Latex string: -10.85$\\pm$0.01\n",
      "\n",
      "5 runs found and evaluated for tretail\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Evaluating dna..\n",
      "\n",
      " --- BMV on TEST ---\n",
      "Evaluating using 128 bins..\n",
      "AVG LL: -98.789055  STD LL: 0.520151 \n",
      "Latex string: -98.79$\\pm$0.52\n",
      "Evaluating using 256 bins..\n",
      "AVG LL: -97.753395  STD LL: 0.270322 \n",
      "Latex string: -97.75$\\pm$0.27\n",
      "Evaluating using 512 bins..\n",
      "AVG LL: -96.670921  STD LL: 0.114077 \n",
      "Latex string: -96.67$\\pm$0.11\n",
      "Evaluating using 1024 bins..\n",
      "AVG LL: -96.135689  STD LL: 0.204996 \n",
      "Latex string: -96.14$\\pm$0.20\n",
      "Evaluating using 2048 bins..\n",
      "AVG LL: -95.891579  STD LL: 0.300091 \n",
      "Latex string: -95.89$\\pm$0.30\n",
      "Evaluating using 4096 bins..\n",
      "AVG LL: -95.810730  STD LL: 0.335595 \n",
      "Latex string: -95.81$\\pm$0.34\n",
      "Evaluating using 8192 bins..\n",
      "AVG LL: -95.693153  STD LL: 0.351375 \n",
      "Latex string: -95.69$\\pm$0.35\n",
      "\n",
      "5 runs found and evaluated for dna\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you run OOM you can tweak n_chunks and batch_size\n",
    "only_test = True\n",
    "n_chunks = None\n",
    "batch_size = 32\n",
    "\n",
    "for dataset_name in DEBD_DATASETS:\n",
    "    \n",
    "    _, valid, test = load_debd(dataset_name)\n",
    "    valid_loader = DataLoader(valid, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size)\n",
    "    print('Evaluating ' + dataset_name + '..')\n",
    "\n",
    "    if not only_test:\n",
    "        bmv_valid_lls_dict = {n_bins: [] for n_bins in n_bins_list}\n",
    "    bmv_test_lls_dict = {n_bins: [] for n_bins in n_bins_list}\n",
    "        \n",
    "    exp_runs = 0\n",
    "    folder_tree = list(os.walk(log_dir + dataset_name))\n",
    "    for folder in folder_tree:\n",
    "        \n",
    "        if 'checkpoints' in folder[0]:\n",
    "            exp_runs += 1\n",
    "            for ckpt in folder[2]:\n",
    "                model = ContinuousMixture.load_from_checkpoint(folder[0] + '/' + ckpt).to(device)\n",
    "                model.n_chunks = n_chunks\n",
    "                model.missing = False\n",
    "                for n_bins in n_bins_list:\n",
    "                    test_sampler = GaussianQMCSampler(latent_dim=4, n_bins=n_bins)\n",
    "                    z, log_w = test_sampler(seed=42)\n",
    "                    if 'best_model_valid' in ckpt:\n",
    "                        if not only_test:\n",
    "                            bmv_valid_lls_dict[n_bins].append(\n",
    "                                model.eval_loader(valid_loader, z, log_w, device=device).cpu().numpy())\n",
    "                        bmv_test_lls_dict[n_bins].append(\n",
    "                            model.eval_loader(test_loader, z, log_w, device=device).cpu().numpy())\n",
    "\n",
    "    if not only_test:\n",
    "        print('\\n --- BMV on VALID ---')\n",
    "        evaluate_lls_dict(bmv_valid_lls_dict)\n",
    "    print('\\n --- BMV on TEST ---')\n",
    "    evaluate_lls_dict(bmv_test_lls_dict)\n",
    "    \n",
    "    print('\\n' + str(exp_runs) + ' runs found and evaluated for ' + dataset_name + '\\n\\n')\n",
    "    print('---------------------------------------------------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
