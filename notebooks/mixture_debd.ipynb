{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79cd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "repo_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ade8f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils.reproducibility import seed_everything\n",
    "from models.mixtures import BernoulliMixture\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import load_debd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpus = None if device == 'cpu' else 1\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37550bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tmovie'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a402d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmovie (4524, 500) (1002, 500) (591, 500)\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = load_debd(dataset_name)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, drop_last=True)\n",
    "print(dataset_name, train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e9fbe",
   "metadata": {},
   "source": [
    "## Instantiate mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c8b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "n_components = 1024\n",
    "model = BernoulliMixture(\n",
    "    logits_p=torch.randn(n_components, train.shape[1]),\n",
    "    logits_w=torch.full((n_components,), 1 / n_components),\n",
    "    learn_w=False\n",
    ").to(device)\n",
    "opt = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d7fb6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee541fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 600] Training loss: 65.528224 Validation Loss: 71.643715 e: 0\n",
      "Epoch [2 / 600] Training loss: 65.508934 Validation Loss: 71.635198 e: 0\n",
      "Epoch [3 / 600] Training loss: 65.514985 Validation Loss: 71.625743 e: 0\n",
      "Epoch [4 / 600] Training loss: 65.450450 Validation Loss: 71.617835 e: 0\n",
      "Epoch [5 / 600] Training loss: 65.362695 Validation Loss: 71.607473 e: 0\n",
      "Epoch [6 / 600] Training loss: 65.393344 Validation Loss: 71.598733 e: 0\n",
      "Epoch [7 / 600] Training loss: 65.413456 Validation Loss: 71.588818 e: 0\n",
      "Epoch [8 / 600] Training loss: 65.414789 Validation Loss: 71.579820 e: 0\n",
      "Epoch [9 / 600] Training loss: 65.330715 Validation Loss: 71.570221 e: 0\n",
      "Epoch [10 / 600] Training loss: 65.436517 Validation Loss: 71.559003 e: 0\n",
      "Epoch [11 / 600] Training loss: 65.344182 Validation Loss: 71.550657 e: 0\n",
      "Epoch [12 / 600] Training loss: 65.242291 Validation Loss: 71.539564 e: 0\n",
      "Epoch [13 / 600] Training loss: 65.409569 Validation Loss: 71.530572 e: 0\n",
      "Epoch [14 / 600] Training loss: 65.424750 Validation Loss: 71.520691 e: 0\n",
      "Epoch [15 / 600] Training loss: 65.456654 Validation Loss: 71.510476 e: 0\n",
      "Epoch [16 / 600] Training loss: 65.181200 Validation Loss: 71.500357 e: 0\n",
      "Epoch [17 / 600] Training loss: 65.363043 Validation Loss: 71.490189 e: 0\n",
      "Epoch [18 / 600] Training loss: 65.448635 Validation Loss: 71.480559 e: 0\n",
      "Epoch [19 / 600] Training loss: 65.332716 Validation Loss: 71.470826 e: 0\n",
      "Epoch [20 / 600] Training loss: 65.436448 Validation Loss: 71.461311 e: 0\n",
      "Epoch [21 / 600] Training loss: 65.388009 Validation Loss: 71.451630 e: 0\n",
      "Epoch [22 / 600] Training loss: 65.364439 Validation Loss: 71.441754 e: 0\n",
      "Epoch [23 / 600] Training loss: 65.339694 Validation Loss: 71.432698 e: 0\n",
      "Epoch [24 / 600] Training loss: 65.233005 Validation Loss: 71.423724 e: 0\n",
      "Epoch [25 / 600] Training loss: 65.296295 Validation Loss: 71.414370 e: 0\n",
      "Epoch [26 / 600] Training loss: 65.354371 Validation Loss: 71.403695 e: 0\n",
      "Epoch [27 / 600] Training loss: 65.349303 Validation Loss: 71.394015 e: 0\n",
      "Epoch [28 / 600] Training loss: 65.083557 Validation Loss: 71.385276 e: 0\n",
      "Epoch [29 / 600] Training loss: 65.173570 Validation Loss: 71.374682 e: 0\n",
      "Epoch [30 / 600] Training loss: 65.251619 Validation Loss: 71.364447 e: 0\n",
      "Epoch [31 / 600] Training loss: 65.061476 Validation Loss: 71.354760 e: 0\n",
      "Epoch [32 / 600] Training loss: 65.141699 Validation Loss: 71.346188 e: 0\n",
      "Epoch [33 / 600] Training loss: 65.055675 Validation Loss: 71.337303 e: 0\n",
      "Epoch [34 / 600] Training loss: 65.247168 Validation Loss: 71.328414 e: 0\n",
      "Epoch [35 / 600] Training loss: 65.128975 Validation Loss: 71.318850 e: 0\n",
      "Epoch [36 / 600] Training loss: 65.148116 Validation Loss: 71.310299 e: 0\n",
      "Epoch [37 / 600] Training loss: 65.225008 Validation Loss: 71.301189 e: 0\n",
      "Epoch [38 / 600] Training loss: 65.299199 Validation Loss: 71.292070 e: 0\n",
      "Epoch [39 / 600] Training loss: 64.984227 Validation Loss: 71.281596 e: 0\n",
      "Epoch [40 / 600] Training loss: 65.040296 Validation Loss: 71.272259 e: 0\n",
      "Epoch [41 / 600] Training loss: 65.027582 Validation Loss: 71.264426 e: 0\n",
      "Epoch [42 / 600] Training loss: 65.103422 Validation Loss: 71.255580 e: 0\n",
      "Epoch [43 / 600] Training loss: 65.057131 Validation Loss: 71.246869 e: 0\n",
      "Epoch [44 / 600] Training loss: 64.935826 Validation Loss: 71.237759 e: 0\n",
      "Epoch [45 / 600] Training loss: 64.972152 Validation Loss: 71.228780 e: 0\n",
      "Epoch [46 / 600] Training loss: 65.014150 Validation Loss: 71.220548 e: 0\n",
      "Epoch [47 / 600] Training loss: 65.017193 Validation Loss: 71.213026 e: 0\n",
      "Epoch [48 / 600] Training loss: 65.100751 Validation Loss: 71.205418 e: 0\n",
      "Epoch [49 / 600] Training loss: 65.112205 Validation Loss: 71.197849 e: 0\n",
      "Epoch [50 / 600] Training loss: 65.066393 Validation Loss: 71.190624 e: 0\n",
      "Epoch [51 / 600] Training loss: 65.097163 Validation Loss: 71.182871 e: 0\n",
      "Epoch [52 / 600] Training loss: 65.022522 Validation Loss: 71.175540 e: 0\n",
      "Epoch [53 / 600] Training loss: 64.971201 Validation Loss: 71.168648 e: 0\n",
      "Epoch [54 / 600] Training loss: 64.938180 Validation Loss: 71.162334 e: 0\n",
      "Epoch [55 / 600] Training loss: 64.871622 Validation Loss: 71.155474 e: 0\n",
      "Epoch [56 / 600] Training loss: 65.019842 Validation Loss: 71.147933 e: 0\n",
      "Epoch [57 / 600] Training loss: 64.915302 Validation Loss: 71.141809 e: 0\n",
      "Epoch [58 / 600] Training loss: 64.988565 Validation Loss: 71.135816 e: 0\n",
      "Epoch [59 / 600] Training loss: 64.898620 Validation Loss: 71.129882 e: 0\n",
      "Epoch [60 / 600] Training loss: 64.884061 Validation Loss: 71.123066 e: 0\n",
      "Epoch [61 / 600] Training loss: 64.818310 Validation Loss: 71.116920 e: 0\n",
      "Epoch [62 / 600] Training loss: 64.757056 Validation Loss: 71.110859 e: 0\n",
      "Epoch [63 / 600] Training loss: 64.851650 Validation Loss: 71.105667 e: 0\n",
      "Epoch [64 / 600] Training loss: 64.707987 Validation Loss: 71.099582 e: 0\n",
      "Epoch [65 / 600] Training loss: 65.010368 Validation Loss: 71.094310 e: 0\n",
      "Epoch [66 / 600] Training loss: 64.919440 Validation Loss: 71.088169 e: 0\n",
      "Epoch [67 / 600] Training loss: 64.786986 Validation Loss: 71.082324 e: 0\n",
      "Epoch [68 / 600] Training loss: 64.987082 Validation Loss: 71.076475 e: 0\n",
      "Epoch [69 / 600] Training loss: 64.661866 Validation Loss: 71.070519 e: 0\n",
      "Epoch [70 / 600] Training loss: 64.866007 Validation Loss: 71.064315 e: 0\n",
      "Epoch [71 / 600] Training loss: 64.846904 Validation Loss: 71.058066 e: 0\n",
      "Epoch [72 / 600] Training loss: 64.788005 Validation Loss: 71.052660 e: 0\n",
      "Epoch [73 / 600] Training loss: 64.867927 Validation Loss: 71.045301 e: 0\n",
      "Epoch [74 / 600] Training loss: 64.797244 Validation Loss: 71.039203 e: 0\n",
      "Epoch [75 / 600] Training loss: 64.826986 Validation Loss: 71.032908 e: 0\n",
      "Epoch [76 / 600] Training loss: 64.651918 Validation Loss: 71.026397 e: 0\n",
      "Epoch [77 / 600] Training loss: 64.694072 Validation Loss: 71.019647 e: 0\n",
      "Epoch [78 / 600] Training loss: 64.844139 Validation Loss: 71.013687 e: 0\n",
      "Epoch [79 / 600] Training loss: 64.784607 Validation Loss: 71.006960 e: 0\n",
      "Epoch [80 / 600] Training loss: 64.734185 Validation Loss: 71.000504 e: 0\n",
      "Epoch [81 / 600] Training loss: 64.697860 Validation Loss: 70.994177 e: 0\n",
      "Epoch [82 / 600] Training loss: 64.638247 Validation Loss: 70.987262 e: 0\n",
      "Epoch [83 / 600] Training loss: 64.807151 Validation Loss: 70.981768 e: 0\n",
      "Epoch [84 / 600] Training loss: 64.696484 Validation Loss: 70.974658 e: 0\n",
      "Epoch [85 / 600] Training loss: 64.761594 Validation Loss: 70.967971 e: 0\n",
      "Epoch [86 / 600] Training loss: 64.813134 Validation Loss: 70.961855 e: 0\n",
      "Epoch [87 / 600] Training loss: 64.761695 Validation Loss: 70.955546 e: 0\n",
      "Epoch [88 / 600] Training loss: 64.727513 Validation Loss: 70.948048 e: 0\n",
      "Epoch [89 / 600] Training loss: 64.589537 Validation Loss: 70.942232 e: 0\n",
      "Epoch [90 / 600] Training loss: 64.714767 Validation Loss: 70.936470 e: 0\n",
      "Epoch [91 / 600] Training loss: 64.762666 Validation Loss: 70.930072 e: 0\n",
      "Epoch [92 / 600] Training loss: 64.688957 Validation Loss: 70.923114 e: 0\n",
      "Epoch [93 / 600] Training loss: 64.681144 Validation Loss: 70.916462 e: 0\n",
      "Epoch [94 / 600] Training loss: 64.501363 Validation Loss: 70.909696 e: 0\n",
      "Epoch [95 / 600] Training loss: 64.572267 Validation Loss: 70.904237 e: 0\n",
      "Epoch [96 / 600] Training loss: 64.642098 Validation Loss: 70.898003 e: 0\n",
      "Epoch [97 / 600] Training loss: 64.636738 Validation Loss: 70.891847 e: 0\n",
      "Epoch [98 / 600] Training loss: 64.637446 Validation Loss: 70.885475 e: 0\n",
      "Epoch [99 / 600] Training loss: 64.559484 Validation Loss: 70.878506 e: 0\n",
      "Epoch [100 / 600] Training loss: 64.696809 Validation Loss: 70.871940 e: 0\n",
      "Epoch [101 / 600] Training loss: 64.613181 Validation Loss: 70.866090 e: 0\n",
      "Epoch [102 / 600] Training loss: 64.513472 Validation Loss: 70.859703 e: 0\n",
      "Epoch [103 / 600] Training loss: 64.413468 Validation Loss: 70.853147 e: 0\n",
      "Epoch [104 / 600] Training loss: 64.390429 Validation Loss: 70.846761 e: 0\n",
      "Epoch [105 / 600] Training loss: 64.409011 Validation Loss: 70.840417 e: 0\n",
      "Epoch [106 / 600] Training loss: 64.701675 Validation Loss: 70.833936 e: 0\n",
      "Epoch [107 / 600] Training loss: 64.514507 Validation Loss: 70.828521 e: 0\n",
      "Epoch [108 / 600] Training loss: 64.605318 Validation Loss: 70.822134 e: 0\n",
      "Epoch [109 / 600] Training loss: 64.428175 Validation Loss: 70.814986 e: 0\n",
      "Epoch [110 / 600] Training loss: 64.533061 Validation Loss: 70.809621 e: 0\n",
      "Epoch [111 / 600] Training loss: 64.459106 Validation Loss: 70.803456 e: 0\n",
      "Epoch [112 / 600] Training loss: 64.520365 Validation Loss: 70.795463 e: 0\n",
      "Epoch [113 / 600] Training loss: 64.529545 Validation Loss: 70.790438 e: 0\n",
      "Epoch [114 / 600] Training loss: 64.589127 Validation Loss: 70.784725 e: 0\n",
      "Epoch [115 / 600] Training loss: 64.516587 Validation Loss: 70.778294 e: 0\n",
      "Epoch [116 / 600] Training loss: 64.344276 Validation Loss: 70.773447 e: 0\n",
      "Epoch [117 / 600] Training loss: 64.556546 Validation Loss: 70.766423 e: 0\n",
      "Epoch [118 / 600] Training loss: 64.589022 Validation Loss: 70.761078 e: 0\n",
      "Epoch [119 / 600] Training loss: 64.363066 Validation Loss: 70.755374 e: 0\n",
      "Epoch [120 / 600] Training loss: 64.469008 Validation Loss: 70.749865 e: 0\n",
      "Epoch [121 / 600] Training loss: 64.492813 Validation Loss: 70.744251 e: 0\n",
      "Epoch [122 / 600] Training loss: 64.334003 Validation Loss: 70.738311 e: 0\n",
      "Epoch [123 / 600] Training loss: 64.501362 Validation Loss: 70.734311 e: 0\n",
      "Epoch [124 / 600] Training loss: 64.330222 Validation Loss: 70.726946 e: 0\n",
      "Epoch [125 / 600] Training loss: 64.585452 Validation Loss: 70.722045 e: 0\n",
      "Epoch [126 / 600] Training loss: 64.566446 Validation Loss: 70.717931 e: 0\n",
      "Epoch [127 / 600] Training loss: 64.409793 Validation Loss: 70.711936 e: 0\n",
      "Epoch [128 / 600] Training loss: 64.497172 Validation Loss: 70.707723 e: 0\n",
      "Epoch [129 / 600] Training loss: 64.406530 Validation Loss: 70.702655 e: 0\n",
      "Epoch [130 / 600] Training loss: 64.451544 Validation Loss: 70.697693 e: 0\n",
      "Epoch [131 / 600] Training loss: 64.463042 Validation Loss: 70.691766 e: 0\n",
      "Epoch [132 / 600] Training loss: 64.498980 Validation Loss: 70.686509 e: 0\n",
      "Epoch [133 / 600] Training loss: 64.509581 Validation Loss: 70.681910 e: 0\n",
      "Epoch [134 / 600] Training loss: 64.247417 Validation Loss: 70.677235 e: 0\n",
      "Epoch [135 / 600] Training loss: 64.338033 Validation Loss: 70.672203 e: 0\n",
      "Epoch [136 / 600] Training loss: 64.449048 Validation Loss: 70.667441 e: 0\n",
      "Epoch [137 / 600] Training loss: 64.439905 Validation Loss: 70.662723 e: 0\n",
      "Epoch [138 / 600] Training loss: 64.256252 Validation Loss: 70.657257 e: 0\n",
      "Epoch [139 / 600] Training loss: 64.352488 Validation Loss: 70.652408 e: 0\n",
      "Epoch [140 / 600] Training loss: 64.320491 Validation Loss: 70.647647 e: 0\n",
      "Epoch [141 / 600] Training loss: 64.192047 Validation Loss: 70.644100 e: 0\n",
      "Epoch [142 / 600] Training loss: 64.311567 Validation Loss: 70.638670 e: 0\n",
      "Epoch [143 / 600] Training loss: 64.424036 Validation Loss: 70.634381 e: 0\n",
      "Epoch [144 / 600] Training loss: 64.235835 Validation Loss: 70.629720 e: 0\n",
      "Epoch [145 / 600] Training loss: 64.366486 Validation Loss: 70.625132 e: 0\n",
      "Epoch [146 / 600] Training loss: 64.301003 Validation Loss: 70.620479 e: 0\n",
      "Epoch [147 / 600] Training loss: 64.225334 Validation Loss: 70.616458 e: 0\n",
      "Epoch [148 / 600] Training loss: 64.424126 Validation Loss: 70.611336 e: 0\n",
      "Epoch [149 / 600] Training loss: 64.446995 Validation Loss: 70.607632 e: 0\n",
      "Epoch [150 / 600] Training loss: 64.218521 Validation Loss: 70.603293 e: 0\n",
      "Epoch [151 / 600] Training loss: 64.314901 Validation Loss: 70.599133 e: 0\n",
      "Epoch [152 / 600] Training loss: 64.315714 Validation Loss: 70.593713 e: 0\n",
      "Epoch [153 / 600] Training loss: 64.245056 Validation Loss: 70.589674 e: 0\n",
      "Epoch [154 / 600] Training loss: 64.319138 Validation Loss: 70.586138 e: 0\n",
      "Epoch [155 / 600] Training loss: 64.426036 Validation Loss: 70.581037 e: 0\n",
      "Epoch [156 / 600] Training loss: 64.348115 Validation Loss: 70.576795 e: 0\n",
      "Epoch [157 / 600] Training loss: 64.336761 Validation Loss: 70.572813 e: 0\n",
      "Epoch [158 / 600] Training loss: 64.339309 Validation Loss: 70.568734 e: 0\n",
      "Epoch [159 / 600] Training loss: 64.206215 Validation Loss: 70.563959 e: 0\n",
      "Epoch [160 / 600] Training loss: 64.132734 Validation Loss: 70.559279 e: 0\n",
      "Epoch [161 / 600] Training loss: 64.261837 Validation Loss: 70.553893 e: 0\n",
      "Epoch [162 / 600] Training loss: 64.337076 Validation Loss: 70.549674 e: 0\n",
      "Epoch [163 / 600] Training loss: 64.292394 Validation Loss: 70.544969 e: 0\n",
      "Epoch [164 / 600] Training loss: 64.167571 Validation Loss: 70.539300 e: 0\n",
      "Epoch [165 / 600] Training loss: 64.347238 Validation Loss: 70.534719 e: 0\n",
      "Epoch [166 / 600] Training loss: 64.259123 Validation Loss: 70.529190 e: 0\n",
      "Epoch [167 / 600] Training loss: 64.125033 Validation Loss: 70.524457 e: 0\n",
      "Epoch [168 / 600] Training loss: 64.163172 Validation Loss: 70.518777 e: 0\n",
      "Epoch [169 / 600] Training loss: 64.190379 Validation Loss: 70.513519 e: 0\n",
      "Epoch [170 / 600] Training loss: 64.174979 Validation Loss: 70.507191 e: 0\n",
      "Epoch [171 / 600] Training loss: 64.183419 Validation Loss: 70.502812 e: 0\n",
      "Epoch [172 / 600] Training loss: 64.140955 Validation Loss: 70.496398 e: 0\n",
      "Epoch [173 / 600] Training loss: 64.105529 Validation Loss: 70.490694 e: 0\n",
      "Epoch [174 / 600] Training loss: 64.008858 Validation Loss: 70.485720 e: 0\n",
      "Epoch [175 / 600] Training loss: 64.133615 Validation Loss: 70.480379 e: 0\n",
      "Epoch [176 / 600] Training loss: 64.197408 Validation Loss: 70.475285 e: 0\n",
      "Epoch [177 / 600] Training loss: 64.204548 Validation Loss: 70.468809 e: 0\n",
      "Epoch [178 / 600] Training loss: 63.952423 Validation Loss: 70.463260 e: 0\n",
      "Epoch [179 / 600] Training loss: 64.093573 Validation Loss: 70.456900 e: 0\n",
      "Epoch [180 / 600] Training loss: 64.136646 Validation Loss: 70.450850 e: 0\n",
      "Epoch [181 / 600] Training loss: 64.057340 Validation Loss: 70.445872 e: 0\n",
      "Epoch [182 / 600] Training loss: 64.173324 Validation Loss: 70.440020 e: 0\n",
      "Epoch [183 / 600] Training loss: 64.134856 Validation Loss: 70.433586 e: 0\n",
      "Epoch [184 / 600] Training loss: 64.205885 Validation Loss: 70.428675 e: 0\n",
      "Epoch [185 / 600] Training loss: 64.031793 Validation Loss: 70.421408 e: 0\n",
      "Epoch [186 / 600] Training loss: 64.156986 Validation Loss: 70.415365 e: 0\n",
      "Epoch [187 / 600] Training loss: 64.043764 Validation Loss: 70.408848 e: 0\n",
      "Epoch [188 / 600] Training loss: 64.040745 Validation Loss: 70.403095 e: 0\n",
      "Epoch [189 / 600] Training loss: 64.102688 Validation Loss: 70.396866 e: 0\n",
      "Epoch [190 / 600] Training loss: 63.860730 Validation Loss: 70.390077 e: 0\n",
      "Epoch [191 / 600] Training loss: 64.153656 Validation Loss: 70.383242 e: 0\n",
      "Epoch [192 / 600] Training loss: 64.061192 Validation Loss: 70.377169 e: 0\n",
      "Epoch [193 / 600] Training loss: 64.030374 Validation Loss: 70.369778 e: 0\n",
      "Epoch [194 / 600] Training loss: 64.182704 Validation Loss: 70.364747 e: 0\n",
      "Epoch [195 / 600] Training loss: 64.140735 Validation Loss: 70.358148 e: 0\n",
      "Epoch [196 / 600] Training loss: 64.067564 Validation Loss: 70.351869 e: 0\n",
      "Epoch [197 / 600] Training loss: 64.068030 Validation Loss: 70.346095 e: 0\n",
      "Epoch [198 / 600] Training loss: 64.030717 Validation Loss: 70.339316 e: 0\n",
      "Epoch [199 / 600] Training loss: 63.964214 Validation Loss: 70.333156 e: 0\n",
      "Epoch [200 / 600] Training loss: 64.036638 Validation Loss: 70.327519 e: 0\n",
      "Epoch [201 / 600] Training loss: 64.105086 Validation Loss: 70.321699 e: 0\n",
      "Epoch [202 / 600] Training loss: 64.102128 Validation Loss: 70.314644 e: 0\n",
      "Epoch [203 / 600] Training loss: 63.945881 Validation Loss: 70.308467 e: 0\n",
      "Epoch [204 / 600] Training loss: 63.912891 Validation Loss: 70.303521 e: 0\n",
      "Epoch [205 / 600] Training loss: 64.069046 Validation Loss: 70.298445 e: 0\n",
      "Epoch [206 / 600] Training loss: 63.937819 Validation Loss: 70.292079 e: 0\n",
      "Epoch [207 / 600] Training loss: 64.064773 Validation Loss: 70.286074 e: 0\n",
      "Epoch [208 / 600] Training loss: 64.008678 Validation Loss: 70.280076 e: 0\n",
      "Epoch [209 / 600] Training loss: 64.033291 Validation Loss: 70.274131 e: 0\n",
      "Epoch [210 / 600] Training loss: 63.988046 Validation Loss: 70.268191 e: 0\n",
      "Epoch [211 / 600] Training loss: 63.886534 Validation Loss: 70.261852 e: 0\n",
      "Epoch [212 / 600] Training loss: 63.995341 Validation Loss: 70.256739 e: 0\n",
      "Epoch [213 / 600] Training loss: 64.026368 Validation Loss: 70.250161 e: 0\n",
      "Epoch [214 / 600] Training loss: 63.980544 Validation Loss: 70.245026 e: 0\n",
      "Epoch [215 / 600] Training loss: 63.919686 Validation Loss: 70.239374 e: 0\n",
      "Epoch [216 / 600] Training loss: 63.964512 Validation Loss: 70.232840 e: 0\n",
      "Epoch [217 / 600] Training loss: 64.004098 Validation Loss: 70.226850 e: 0\n",
      "Epoch [218 / 600] Training loss: 63.941372 Validation Loss: 70.222528 e: 0\n",
      "Epoch [219 / 600] Training loss: 64.017316 Validation Loss: 70.216900 e: 0\n",
      "Epoch [220 / 600] Training loss: 63.928355 Validation Loss: 70.211932 e: 0\n",
      "Epoch [221 / 600] Training loss: 63.957214 Validation Loss: 70.206058 e: 0\n",
      "Epoch [222 / 600] Training loss: 63.898002 Validation Loss: 70.200516 e: 0\n",
      "Epoch [223 / 600] Training loss: 63.861313 Validation Loss: 70.194583 e: 0\n",
      "Epoch [224 / 600] Training loss: 63.846123 Validation Loss: 70.189432 e: 0\n",
      "Epoch [225 / 600] Training loss: 63.777914 Validation Loss: 70.184545 e: 0\n",
      "Epoch [226 / 600] Training loss: 63.835139 Validation Loss: 70.179613 e: 0\n",
      "Epoch [227 / 600] Training loss: 63.945430 Validation Loss: 70.173332 e: 0\n",
      "Epoch [228 / 600] Training loss: 63.901760 Validation Loss: 70.169833 e: 0\n",
      "Epoch [229 / 600] Training loss: 63.907274 Validation Loss: 70.164010 e: 0\n",
      "Epoch [230 / 600] Training loss: 63.829492 Validation Loss: 70.159515 e: 0\n",
      "Epoch [231 / 600] Training loss: 63.902391 Validation Loss: 70.153676 e: 0\n",
      "Epoch [232 / 600] Training loss: 63.801454 Validation Loss: 70.149394 e: 0\n",
      "Epoch [233 / 600] Training loss: 63.922111 Validation Loss: 70.144099 e: 0\n",
      "Epoch [234 / 600] Training loss: 63.887382 Validation Loss: 70.139451 e: 0\n",
      "Epoch [235 / 600] Training loss: 63.830768 Validation Loss: 70.133172 e: 0\n",
      "Epoch [236 / 600] Training loss: 63.780273 Validation Loss: 70.128006 e: 0\n",
      "Epoch [237 / 600] Training loss: 63.798457 Validation Loss: 70.122136 e: 0\n",
      "Epoch [238 / 600] Training loss: 63.879226 Validation Loss: 70.116123 e: 0\n",
      "Epoch [239 / 600] Training loss: 63.757277 Validation Loss: 70.112104 e: 0\n",
      "Epoch [240 / 600] Training loss: 63.933227 Validation Loss: 70.106644 e: 0\n",
      "Epoch [241 / 600] Training loss: 63.871474 Validation Loss: 70.101206 e: 0\n",
      "Epoch [242 / 600] Training loss: 63.954983 Validation Loss: 70.096673 e: 0\n",
      "Epoch [243 / 600] Training loss: 63.773447 Validation Loss: 70.090687 e: 0\n",
      "Epoch [244 / 600] Training loss: 63.929258 Validation Loss: 70.086276 e: 0\n",
      "Epoch [245 / 600] Training loss: 63.744073 Validation Loss: 70.081028 e: 0\n",
      "Epoch [246 / 600] Training loss: 63.867921 Validation Loss: 70.074315 e: 0\n",
      "Epoch [247 / 600] Training loss: 63.890203 Validation Loss: 70.069614 e: 0\n",
      "Epoch [248 / 600] Training loss: 63.697890 Validation Loss: 70.063931 e: 0\n",
      "Epoch [249 / 600] Training loss: 63.847190 Validation Loss: 70.059757 e: 0\n",
      "Epoch [250 / 600] Training loss: 63.738001 Validation Loss: 70.054003 e: 0\n",
      "Epoch [251 / 600] Training loss: 63.779914 Validation Loss: 70.049205 e: 0\n",
      "Epoch [252 / 600] Training loss: 63.697938 Validation Loss: 70.044830 e: 0\n",
      "Epoch [253 / 600] Training loss: 63.734495 Validation Loss: 70.039754 e: 0\n",
      "Epoch [254 / 600] Training loss: 63.735453 Validation Loss: 70.034693 e: 0\n",
      "Epoch [255 / 600] Training loss: 63.818505 Validation Loss: 70.030222 e: 0\n",
      "Epoch [256 / 600] Training loss: 63.845324 Validation Loss: 70.024228 e: 0\n",
      "Epoch [257 / 600] Training loss: 63.712706 Validation Loss: 70.019305 e: 0\n",
      "Epoch [258 / 600] Training loss: 63.883528 Validation Loss: 70.013952 e: 0\n",
      "Epoch [259 / 600] Training loss: 63.613571 Validation Loss: 70.008319 e: 0\n",
      "Epoch [260 / 600] Training loss: 63.673189 Validation Loss: 70.003517 e: 0\n",
      "Epoch [261 / 600] Training loss: 63.651368 Validation Loss: 69.997136 e: 0\n",
      "Epoch [262 / 600] Training loss: 63.649221 Validation Loss: 69.992388 e: 0\n",
      "Epoch [263 / 600] Training loss: 63.865095 Validation Loss: 69.986439 e: 0\n",
      "Epoch [264 / 600] Training loss: 63.834575 Validation Loss: 69.981935 e: 0\n",
      "Epoch [265 / 600] Training loss: 63.692713 Validation Loss: 69.975840 e: 0\n",
      "Epoch [266 / 600] Training loss: 63.565846 Validation Loss: 69.969866 e: 0\n",
      "Epoch [267 / 600] Training loss: 63.852619 Validation Loss: 69.964665 e: 0\n",
      "Epoch [268 / 600] Training loss: 63.722327 Validation Loss: 69.957959 e: 0\n",
      "Epoch [269 / 600] Training loss: 63.599641 Validation Loss: 69.952232 e: 0\n",
      "Epoch [270 / 600] Training loss: 63.612964 Validation Loss: 69.946529 e: 0\n",
      "Epoch [271 / 600] Training loss: 63.685604 Validation Loss: 69.939861 e: 0\n",
      "Epoch [272 / 600] Training loss: 63.667907 Validation Loss: 69.934189 e: 0\n",
      "Epoch [273 / 600] Training loss: 63.574897 Validation Loss: 69.926870 e: 0\n",
      "Epoch [274 / 600] Training loss: 63.619721 Validation Loss: 69.921332 e: 0\n",
      "Epoch [275 / 600] Training loss: 63.636166 Validation Loss: 69.915349 e: 0\n",
      "Epoch [276 / 600] Training loss: 63.616865 Validation Loss: 69.908323 e: 0\n",
      "Epoch [277 / 600] Training loss: 63.614175 Validation Loss: 69.902570 e: 0\n",
      "Epoch [278 / 600] Training loss: 63.688454 Validation Loss: 69.895967 e: 0\n",
      "Epoch [279 / 600] Training loss: 63.685063 Validation Loss: 69.889316 e: 0\n",
      "Epoch [280 / 600] Training loss: 63.622826 Validation Loss: 69.882338 e: 0\n",
      "Epoch [281 / 600] Training loss: 63.571124 Validation Loss: 69.876059 e: 0\n",
      "Epoch [282 / 600] Training loss: 63.624925 Validation Loss: 69.869485 e: 0\n",
      "Epoch [283 / 600] Training loss: 63.526511 Validation Loss: 69.862407 e: 0\n",
      "Epoch [284 / 600] Training loss: 63.668918 Validation Loss: 69.856076 e: 0\n",
      "Epoch [285 / 600] Training loss: 63.481522 Validation Loss: 69.848628 e: 0\n",
      "Epoch [286 / 600] Training loss: 63.556992 Validation Loss: 69.841129 e: 0\n",
      "Epoch [287 / 600] Training loss: 63.665479 Validation Loss: 69.835817 e: 0\n",
      "Epoch [288 / 600] Training loss: 63.426259 Validation Loss: 69.828942 e: 0\n",
      "Epoch [289 / 600] Training loss: 63.565528 Validation Loss: 69.822146 e: 0\n",
      "Epoch [290 / 600] Training loss: 63.542774 Validation Loss: 69.815733 e: 0\n",
      "Epoch [291 / 600] Training loss: 63.518306 Validation Loss: 69.809455 e: 0\n",
      "Epoch [292 / 600] Training loss: 63.466800 Validation Loss: 69.803432 e: 0\n",
      "Epoch [293 / 600] Training loss: 63.525826 Validation Loss: 69.797334 e: 0\n",
      "Epoch [294 / 600] Training loss: 63.415204 Validation Loss: 69.791478 e: 0\n",
      "Epoch [295 / 600] Training loss: 63.372187 Validation Loss: 69.785039 e: 0\n",
      "Epoch [296 / 600] Training loss: 63.646789 Validation Loss: 69.779494 e: 0\n",
      "Epoch [297 / 600] Training loss: 63.503016 Validation Loss: 69.773717 e: 0\n",
      "Epoch [298 / 600] Training loss: 63.580508 Validation Loss: 69.767581 e: 0\n",
      "Epoch [299 / 600] Training loss: 63.527478 Validation Loss: 69.761941 e: 0\n",
      "Epoch [300 / 600] Training loss: 63.608004 Validation Loss: 69.755290 e: 0\n",
      "Epoch [301 / 600] Training loss: 63.416388 Validation Loss: 69.749063 e: 0\n",
      "Epoch [302 / 600] Training loss: 63.517864 Validation Loss: 69.743620 e: 0\n",
      "Epoch [303 / 600] Training loss: 63.394806 Validation Loss: 69.737521 e: 0\n",
      "Epoch [304 / 600] Training loss: 63.392492 Validation Loss: 69.730346 e: 0\n",
      "Epoch [305 / 600] Training loss: 63.393183 Validation Loss: 69.724890 e: 0\n",
      "Epoch [306 / 600] Training loss: 63.442861 Validation Loss: 69.718541 e: 0\n",
      "Epoch [307 / 600] Training loss: 63.474662 Validation Loss: 69.712793 e: 0\n",
      "Epoch [308 / 600] Training loss: 63.374403 Validation Loss: 69.705801 e: 0\n",
      "Epoch [309 / 600] Training loss: 63.456994 Validation Loss: 69.698565 e: 0\n",
      "Epoch [310 / 600] Training loss: 63.259415 Validation Loss: 69.692232 e: 0\n",
      "Epoch [311 / 600] Training loss: 63.408283 Validation Loss: 69.686736 e: 0\n",
      "Epoch [312 / 600] Training loss: 63.418703 Validation Loss: 69.679961 e: 0\n",
      "Epoch [313 / 600] Training loss: 63.379429 Validation Loss: 69.673279 e: 0\n",
      "Epoch [314 / 600] Training loss: 63.341743 Validation Loss: 69.666514 e: 0\n",
      "Epoch [315 / 600] Training loss: 63.368609 Validation Loss: 69.659957 e: 0\n",
      "Epoch [316 / 600] Training loss: 63.408828 Validation Loss: 69.653379 e: 0\n",
      "Epoch [317 / 600] Training loss: 63.370703 Validation Loss: 69.647440 e: 0\n",
      "Epoch [318 / 600] Training loss: 63.406549 Validation Loss: 69.639955 e: 0\n",
      "Epoch [319 / 600] Training loss: 63.342583 Validation Loss: 69.634019 e: 0\n",
      "Epoch [320 / 600] Training loss: 63.347028 Validation Loss: 69.627014 e: 0\n",
      "Epoch [321 / 600] Training loss: 63.327795 Validation Loss: 69.619720 e: 0\n",
      "Epoch [322 / 600] Training loss: 63.478970 Validation Loss: 69.612347 e: 0\n",
      "Epoch [323 / 600] Training loss: 63.262087 Validation Loss: 69.604729 e: 0\n",
      "Epoch [324 / 600] Training loss: 63.316484 Validation Loss: 69.597541 e: 0\n",
      "Epoch [325 / 600] Training loss: 63.264227 Validation Loss: 69.590735 e: 0\n",
      "Epoch [326 / 600] Training loss: 63.329375 Validation Loss: 69.583211 e: 0\n",
      "Epoch [327 / 600] Training loss: 63.330746 Validation Loss: 69.576212 e: 0\n",
      "Epoch [328 / 600] Training loss: 63.268670 Validation Loss: 69.568491 e: 0\n",
      "Epoch [329 / 600] Training loss: 63.275045 Validation Loss: 69.561042 e: 0\n",
      "Epoch [330 / 600] Training loss: 63.338472 Validation Loss: 69.553349 e: 0\n",
      "Epoch [331 / 600] Training loss: 63.361464 Validation Loss: 69.546612 e: 0\n",
      "Epoch [332 / 600] Training loss: 63.334871 Validation Loss: 69.539198 e: 0\n",
      "Epoch [333 / 600] Training loss: 63.415701 Validation Loss: 69.531593 e: 0\n",
      "Epoch [334 / 600] Training loss: 63.354248 Validation Loss: 69.524077 e: 0\n",
      "Epoch [335 / 600] Training loss: 63.264387 Validation Loss: 69.515360 e: 0\n",
      "Epoch [336 / 600] Training loss: 63.292441 Validation Loss: 69.507999 e: 0\n",
      "Epoch [337 / 600] Training loss: 63.135109 Validation Loss: 69.500404 e: 0\n",
      "Epoch [338 / 600] Training loss: 63.310656 Validation Loss: 69.493025 e: 0\n",
      "Epoch [339 / 600] Training loss: 63.244007 Validation Loss: 69.486087 e: 0\n",
      "Epoch [340 / 600] Training loss: 63.155743 Validation Loss: 69.477943 e: 0\n",
      "Epoch [341 / 600] Training loss: 63.378474 Validation Loss: 69.470721 e: 0\n",
      "Epoch [342 / 600] Training loss: 63.333712 Validation Loss: 69.463329 e: 0\n",
      "Epoch [343 / 600] Training loss: 63.100734 Validation Loss: 69.456329 e: 0\n",
      "Epoch [344 / 600] Training loss: 63.262885 Validation Loss: 69.449545 e: 0\n",
      "Epoch [345 / 600] Training loss: 63.293573 Validation Loss: 69.442480 e: 0\n",
      "Epoch [346 / 600] Training loss: 63.224318 Validation Loss: 69.435019 e: 0\n",
      "Epoch [347 / 600] Training loss: 63.261136 Validation Loss: 69.428571 e: 0\n",
      "Epoch [348 / 600] Training loss: 63.071989 Validation Loss: 69.420666 e: 0\n",
      "Epoch [349 / 600] Training loss: 63.135366 Validation Loss: 69.414201 e: 0\n",
      "Epoch [350 / 600] Training loss: 63.230110 Validation Loss: 69.407413 e: 0\n",
      "Epoch [351 / 600] Training loss: 63.196082 Validation Loss: 69.400308 e: 0\n",
      "Epoch [352 / 600] Training loss: 63.144500 Validation Loss: 69.394190 e: 0\n",
      "Epoch [353 / 600] Training loss: 63.129204 Validation Loss: 69.386380 e: 0\n",
      "Epoch [354 / 600] Training loss: 63.152367 Validation Loss: 69.380131 e: 0\n",
      "Epoch [355 / 600] Training loss: 63.152688 Validation Loss: 69.372974 e: 0\n",
      "Epoch [356 / 600] Training loss: 63.258153 Validation Loss: 69.365439 e: 0\n",
      "Epoch [357 / 600] Training loss: 63.064816 Validation Loss: 69.359648 e: 0\n",
      "Epoch [358 / 600] Training loss: 63.110428 Validation Loss: 69.352705 e: 0\n",
      "Epoch [359 / 600] Training loss: 63.072871 Validation Loss: 69.345666 e: 0\n",
      "Epoch [360 / 600] Training loss: 63.118723 Validation Loss: 69.339478 e: 0\n",
      "Epoch [361 / 600] Training loss: 63.017682 Validation Loss: 69.331908 e: 0\n",
      "Epoch [362 / 600] Training loss: 63.231785 Validation Loss: 69.325612 e: 0\n",
      "Epoch [363 / 600] Training loss: 63.175948 Validation Loss: 69.318293 e: 0\n",
      "Epoch [364 / 600] Training loss: 63.082746 Validation Loss: 69.311798 e: 0\n",
      "Epoch [365 / 600] Training loss: 63.036041 Validation Loss: 69.304124 e: 0\n",
      "Epoch [366 / 600] Training loss: 62.986131 Validation Loss: 69.298290 e: 0\n",
      "Epoch [367 / 600] Training loss: 63.032084 Validation Loss: 69.291433 e: 0\n",
      "Epoch [368 / 600] Training loss: 62.879773 Validation Loss: 69.285172 e: 0\n",
      "Epoch [369 / 600] Training loss: 63.196218 Validation Loss: 69.278982 e: 0\n",
      "Epoch [370 / 600] Training loss: 63.099468 Validation Loss: 69.272198 e: 0\n",
      "Epoch [371 / 600] Training loss: 63.058403 Validation Loss: 69.266120 e: 0\n",
      "Epoch [372 / 600] Training loss: 62.998124 Validation Loss: 69.259691 e: 0\n",
      "Epoch [373 / 600] Training loss: 62.963862 Validation Loss: 69.253490 e: 0\n",
      "Epoch [374 / 600] Training loss: 63.184372 Validation Loss: 69.247857 e: 0\n",
      "Epoch [375 / 600] Training loss: 62.966504 Validation Loss: 69.241423 e: 0\n",
      "Epoch [376 / 600] Training loss: 63.053757 Validation Loss: 69.234944 e: 0\n",
      "Epoch [377 / 600] Training loss: 62.975425 Validation Loss: 69.229640 e: 0\n",
      "Epoch [378 / 600] Training loss: 63.061982 Validation Loss: 69.223610 e: 0\n",
      "Epoch [379 / 600] Training loss: 62.958257 Validation Loss: 69.217855 e: 0\n",
      "Epoch [380 / 600] Training loss: 63.017187 Validation Loss: 69.212339 e: 0\n",
      "Epoch [381 / 600] Training loss: 63.064899 Validation Loss: 69.206977 e: 0\n",
      "Epoch [382 / 600] Training loss: 62.955517 Validation Loss: 69.200862 e: 0\n",
      "Epoch [383 / 600] Training loss: 62.945049 Validation Loss: 69.195390 e: 0\n",
      "Epoch [384 / 600] Training loss: 63.012536 Validation Loss: 69.190017 e: 0\n",
      "Epoch [385 / 600] Training loss: 62.983422 Validation Loss: 69.185182 e: 0\n",
      "Epoch [386 / 600] Training loss: 62.948783 Validation Loss: 69.179795 e: 0\n",
      "Epoch [387 / 600] Training loss: 63.013655 Validation Loss: 69.175049 e: 0\n",
      "Epoch [388 / 600] Training loss: 62.996046 Validation Loss: 69.170062 e: 0\n",
      "Epoch [389 / 600] Training loss: 62.934468 Validation Loss: 69.165154 e: 0\n",
      "Epoch [390 / 600] Training loss: 63.063185 Validation Loss: 69.160677 e: 0\n",
      "Epoch [391 / 600] Training loss: 62.995472 Validation Loss: 69.155609 e: 0\n",
      "Epoch [392 / 600] Training loss: 62.844312 Validation Loss: 69.151357 e: 0\n",
      "Epoch [393 / 600] Training loss: 62.922053 Validation Loss: 69.147012 e: 0\n",
      "Epoch [394 / 600] Training loss: 62.923185 Validation Loss: 69.142882 e: 0\n",
      "Epoch [395 / 600] Training loss: 62.789068 Validation Loss: 69.138515 e: 0\n",
      "Epoch [396 / 600] Training loss: 62.937730 Validation Loss: 69.133832 e: 0\n",
      "Epoch [397 / 600] Training loss: 62.833084 Validation Loss: 69.129741 e: 0\n",
      "Epoch [398 / 600] Training loss: 62.937686 Validation Loss: 69.126170 e: 0\n",
      "Epoch [399 / 600] Training loss: 62.759323 Validation Loss: 69.121319 e: 0\n",
      "Epoch [400 / 600] Training loss: 62.778374 Validation Loss: 69.116430 e: 0\n",
      "Epoch [401 / 600] Training loss: 62.943123 Validation Loss: 69.113048 e: 0\n",
      "Epoch [402 / 600] Training loss: 62.941606 Validation Loss: 69.108924 e: 0\n",
      "Epoch [403 / 600] Training loss: 62.789383 Validation Loss: 69.105532 e: 0\n",
      "Epoch [404 / 600] Training loss: 62.895336 Validation Loss: 69.101361 e: 0\n",
      "Epoch [405 / 600] Training loss: 62.939337 Validation Loss: 69.098125 e: 0\n",
      "Epoch [406 / 600] Training loss: 62.980762 Validation Loss: 69.094369 e: 0\n",
      "Epoch [407 / 600] Training loss: 62.849095 Validation Loss: 69.090117 e: 0\n",
      "Epoch [408 / 600] Training loss: 62.881171 Validation Loss: 69.086337 e: 0\n",
      "Epoch [409 / 600] Training loss: 62.883108 Validation Loss: 69.083070 e: 0\n",
      "Epoch [410 / 600] Training loss: 62.963507 Validation Loss: 69.078738 e: 0\n",
      "Epoch [411 / 600] Training loss: 62.832655 Validation Loss: 69.075992 e: 0\n",
      "Epoch [412 / 600] Training loss: 62.803313 Validation Loss: 69.072741 e: 0\n",
      "Epoch [413 / 600] Training loss: 62.855673 Validation Loss: 69.068014 e: 0\n",
      "Epoch [414 / 600] Training loss: 62.828909 Validation Loss: 69.064430 e: 0\n",
      "Epoch [415 / 600] Training loss: 62.846987 Validation Loss: 69.061100 e: 0\n",
      "Epoch [416 / 600] Training loss: 62.933569 Validation Loss: 69.056511 e: 0\n",
      "Epoch [417 / 600] Training loss: 62.830268 Validation Loss: 69.053516 e: 0\n",
      "Epoch [418 / 600] Training loss: 62.980207 Validation Loss: 69.049322 e: 0\n",
      "Epoch [419 / 600] Training loss: 62.677611 Validation Loss: 69.044641 e: 0\n",
      "Epoch [420 / 600] Training loss: 62.769023 Validation Loss: 69.041200 e: 0\n",
      "Epoch [421 / 600] Training loss: 62.857392 Validation Loss: 69.036849 e: 0\n",
      "Epoch [422 / 600] Training loss: 62.840085 Validation Loss: 69.033542 e: 0\n",
      "Epoch [423 / 600] Training loss: 62.824187 Validation Loss: 69.029070 e: 0\n",
      "Epoch [424 / 600] Training loss: 62.860658 Validation Loss: 69.024265 e: 0\n",
      "Epoch [425 / 600] Training loss: 62.775464 Validation Loss: 69.020788 e: 0\n",
      "Epoch [426 / 600] Training loss: 62.788416 Validation Loss: 69.016804 e: 0\n",
      "Epoch [427 / 600] Training loss: 62.885496 Validation Loss: 69.012277 e: 0\n",
      "Epoch [428 / 600] Training loss: 62.909311 Validation Loss: 69.007832 e: 0\n",
      "Epoch [429 / 600] Training loss: 62.778177 Validation Loss: 69.004374 e: 0\n",
      "Epoch [430 / 600] Training loss: 62.757717 Validation Loss: 69.000466 e: 0\n",
      "Epoch [431 / 600] Training loss: 62.886782 Validation Loss: 68.996068 e: 0\n",
      "Epoch [432 / 600] Training loss: 62.706259 Validation Loss: 68.992544 e: 0\n",
      "Epoch [433 / 600] Training loss: 62.831446 Validation Loss: 68.988129 e: 0\n",
      "Epoch [434 / 600] Training loss: 62.811991 Validation Loss: 68.984274 e: 0\n",
      "Epoch [435 / 600] Training loss: 62.667752 Validation Loss: 68.980391 e: 0\n",
      "Epoch [436 / 600] Training loss: 62.770016 Validation Loss: 68.977147 e: 0\n",
      "Epoch [437 / 600] Training loss: 62.769174 Validation Loss: 68.973505 e: 0\n",
      "Epoch [438 / 600] Training loss: 62.696308 Validation Loss: 68.969377 e: 0\n",
      "Epoch [439 / 600] Training loss: 62.673348 Validation Loss: 68.965909 e: 0\n",
      "Epoch [440 / 600] Training loss: 62.834296 Validation Loss: 68.962682 e: 0\n",
      "Epoch [441 / 600] Training loss: 62.906162 Validation Loss: 68.958758 e: 0\n",
      "Epoch [442 / 600] Training loss: 62.756419 Validation Loss: 68.955427 e: 0\n",
      "Epoch [443 / 600] Training loss: 62.678428 Validation Loss: 68.951700 e: 0\n",
      "Epoch [444 / 600] Training loss: 62.791876 Validation Loss: 68.947928 e: 0\n",
      "Epoch [445 / 600] Training loss: 62.713536 Validation Loss: 68.944562 e: 0\n",
      "Epoch [446 / 600] Training loss: 62.680749 Validation Loss: 68.940908 e: 0\n",
      "Epoch [447 / 600] Training loss: 62.806406 Validation Loss: 68.937149 e: 0\n",
      "Epoch [448 / 600] Training loss: 62.731790 Validation Loss: 68.933925 e: 0\n",
      "Epoch [449 / 600] Training loss: 62.741791 Validation Loss: 68.930792 e: 0\n",
      "Epoch [450 / 600] Training loss: 62.472214 Validation Loss: 68.926777 e: 0\n",
      "Epoch [451 / 600] Training loss: 62.733805 Validation Loss: 68.922907 e: 0\n",
      "Epoch [452 / 600] Training loss: 62.684769 Validation Loss: 68.920334 e: 0\n",
      "Epoch [453 / 600] Training loss: 62.827482 Validation Loss: 68.916665 e: 0\n",
      "Epoch [454 / 600] Training loss: 62.805165 Validation Loss: 68.913900 e: 0\n",
      "Epoch [455 / 600] Training loss: 62.705179 Validation Loss: 68.909800 e: 0\n",
      "Epoch [456 / 600] Training loss: 62.685457 Validation Loss: 68.905369 e: 0\n",
      "Epoch [457 / 600] Training loss: 62.807844 Validation Loss: 68.902558 e: 0\n",
      "Epoch [458 / 600] Training loss: 62.801040 Validation Loss: 68.898863 e: 0\n",
      "Epoch [459 / 600] Training loss: 62.707789 Validation Loss: 68.896023 e: 0\n",
      "Epoch [460 / 600] Training loss: 62.533094 Validation Loss: 68.892420 e: 0\n",
      "Epoch [461 / 600] Training loss: 62.707004 Validation Loss: 68.888699 e: 0\n",
      "Epoch [462 / 600] Training loss: 62.755694 Validation Loss: 68.886042 e: 0\n",
      "Epoch [463 / 600] Training loss: 62.563885 Validation Loss: 68.882687 e: 0\n",
      "Epoch [464 / 600] Training loss: 62.643833 Validation Loss: 68.879205 e: 0\n",
      "Epoch [465 / 600] Training loss: 62.771371 Validation Loss: 68.875493 e: 0\n",
      "Epoch [466 / 600] Training loss: 62.703669 Validation Loss: 68.873269 e: 0\n",
      "Epoch [467 / 600] Training loss: 62.744963 Validation Loss: 68.868811 e: 0\n",
      "Epoch [468 / 600] Training loss: 62.646532 Validation Loss: 68.866060 e: 0\n",
      "Epoch [469 / 600] Training loss: 62.720363 Validation Loss: 68.864157 e: 0\n",
      "Epoch [470 / 600] Training loss: 62.599722 Validation Loss: 68.860772 e: 0\n",
      "Epoch [471 / 600] Training loss: 62.670780 Validation Loss: 68.857954 e: 0\n",
      "Epoch [472 / 600] Training loss: 62.712615 Validation Loss: 68.854798 e: 0\n",
      "Epoch [473 / 600] Training loss: 62.610708 Validation Loss: 68.851626 e: 0\n",
      "Epoch [474 / 600] Training loss: 62.678379 Validation Loss: 68.848968 e: 0\n",
      "Epoch [475 / 600] Training loss: 62.686721 Validation Loss: 68.847039 e: 0\n",
      "Epoch [476 / 600] Training loss: 62.643574 Validation Loss: 68.844517 e: 0\n",
      "Epoch [477 / 600] Training loss: 62.587512 Validation Loss: 68.841923 e: 0\n",
      "Epoch [478 / 600] Training loss: 62.747338 Validation Loss: 68.839166 e: 0\n",
      "Epoch [479 / 600] Training loss: 62.661711 Validation Loss: 68.836516 e: 0\n",
      "Epoch [480 / 600] Training loss: 62.663654 Validation Loss: 68.834191 e: 0\n",
      "Epoch [481 / 600] Training loss: 62.600067 Validation Loss: 68.831733 e: 0\n",
      "Epoch [482 / 600] Training loss: 62.597299 Validation Loss: 68.828496 e: 0\n",
      "Epoch [483 / 600] Training loss: 62.601692 Validation Loss: 68.826368 e: 0\n",
      "Epoch [484 / 600] Training loss: 62.695722 Validation Loss: 68.823528 e: 0\n",
      "Epoch [485 / 600] Training loss: 62.738612 Validation Loss: 68.820874 e: 0\n",
      "Epoch [486 / 600] Training loss: 62.712365 Validation Loss: 68.818639 e: 0\n",
      "Epoch [487 / 600] Training loss: 62.743361 Validation Loss: 68.816123 e: 0\n",
      "Epoch [488 / 600] Training loss: 62.702653 Validation Loss: 68.814330 e: 0\n",
      "Epoch [489 / 600] Training loss: 62.505529 Validation Loss: 68.811839 e: 0\n",
      "Epoch [490 / 600] Training loss: 62.741127 Validation Loss: 68.809346 e: 0\n",
      "Epoch [491 / 600] Training loss: 62.732615 Validation Loss: 68.807719 e: 0\n",
      "Epoch [492 / 600] Training loss: 62.703881 Validation Loss: 68.805031 e: 0\n",
      "Epoch [493 / 600] Training loss: 62.563749 Validation Loss: 68.802886 e: 0\n",
      "Epoch [494 / 600] Training loss: 62.575403 Validation Loss: 68.800382 e: 0\n",
      "Epoch [495 / 600] Training loss: 62.717309 Validation Loss: 68.797565 e: 0\n",
      "Epoch [496 / 600] Training loss: 62.614029 Validation Loss: 68.796109 e: 0\n",
      "Epoch [497 / 600] Training loss: 62.490971 Validation Loss: 68.793894 e: 0\n",
      "Epoch [498 / 600] Training loss: 62.686379 Validation Loss: 68.792264 e: 0\n",
      "Epoch [499 / 600] Training loss: 62.635239 Validation Loss: 68.789370 e: 0\n",
      "Epoch [500 / 600] Training loss: 62.624000 Validation Loss: 68.787425 e: 0\n",
      "Epoch [501 / 600] Training loss: 62.657410 Validation Loss: 68.784341 e: 0\n",
      "Epoch [502 / 600] Training loss: 62.584609 Validation Loss: 68.782185 e: 0\n",
      "Epoch [503 / 600] Training loss: 62.672082 Validation Loss: 68.780914 e: 0\n",
      "Epoch [504 / 600] Training loss: 62.614565 Validation Loss: 68.778808 e: 0\n",
      "Epoch [505 / 600] Training loss: 62.704134 Validation Loss: 68.776321 e: 0\n",
      "Epoch [506 / 600] Training loss: 62.569961 Validation Loss: 68.773946 e: 0\n",
      "Epoch [507 / 600] Training loss: 62.498993 Validation Loss: 68.771355 e: 0\n",
      "Epoch [508 / 600] Training loss: 62.697085 Validation Loss: 68.768992 e: 0\n",
      "Epoch [509 / 600] Training loss: 62.630352 Validation Loss: 68.767986 e: 0\n",
      "Epoch [510 / 600] Training loss: 62.557310 Validation Loss: 68.765582 e: 0\n",
      "Epoch [511 / 600] Training loss: 62.599230 Validation Loss: 68.763327 e: 0\n",
      "Epoch [512 / 600] Training loss: 62.501336 Validation Loss: 68.760717 e: 0\n",
      "Epoch [513 / 600] Training loss: 62.528776 Validation Loss: 68.759638 e: 0\n",
      "Epoch [514 / 600] Training loss: 62.620847 Validation Loss: 68.757525 e: 0\n",
      "Epoch [515 / 600] Training loss: 62.534879 Validation Loss: 68.755427 e: 0\n",
      "Epoch [516 / 600] Training loss: 62.590310 Validation Loss: 68.753293 e: 0\n",
      "Epoch [517 / 600] Training loss: 62.437095 Validation Loss: 68.751175 e: 0\n",
      "Epoch [518 / 600] Training loss: 62.663688 Validation Loss: 68.748796 e: 0\n",
      "Epoch [519 / 600] Training loss: 62.623863 Validation Loss: 68.747657 e: 0\n",
      "Epoch [520 / 600] Training loss: 62.677331 Validation Loss: 68.745385 e: 0\n",
      "Epoch [521 / 600] Training loss: 62.660915 Validation Loss: 68.743096 e: 0\n",
      "Epoch [522 / 600] Training loss: 62.574879 Validation Loss: 68.740548 e: 0\n",
      "Epoch [523 / 600] Training loss: 62.607270 Validation Loss: 68.738873 e: 0\n",
      "Epoch [524 / 600] Training loss: 62.623829 Validation Loss: 68.737063 e: 0\n",
      "Epoch [525 / 600] Training loss: 62.521545 Validation Loss: 68.735104 e: 0\n",
      "Epoch [526 / 600] Training loss: 62.588761 Validation Loss: 68.732564 e: 0\n",
      "Epoch [527 / 600] Training loss: 62.597638 Validation Loss: 68.730568 e: 0\n",
      "Epoch [528 / 600] Training loss: 62.614288 Validation Loss: 68.727893 e: 0\n",
      "Epoch [529 / 600] Training loss: 62.426706 Validation Loss: 68.726377 e: 0\n",
      "Epoch [530 / 600] Training loss: 62.601584 Validation Loss: 68.723958 e: 0\n",
      "Epoch [531 / 600] Training loss: 62.597855 Validation Loss: 68.723087 e: 0\n",
      "Epoch [532 / 600] Training loss: 62.588450 Validation Loss: 68.720093 e: 0\n",
      "Epoch [533 / 600] Training loss: 62.554600 Validation Loss: 68.718697 e: 0\n",
      "Epoch [534 / 600] Training loss: 62.687454 Validation Loss: 68.716743 e: 0\n",
      "Epoch [535 / 600] Training loss: 62.623644 Validation Loss: 68.714359 e: 0\n",
      "Epoch [536 / 600] Training loss: 62.661666 Validation Loss: 68.712775 e: 0\n",
      "Epoch [537 / 600] Training loss: 62.495723 Validation Loss: 68.710457 e: 0\n",
      "Epoch [538 / 600] Training loss: 62.573786 Validation Loss: 68.708207 e: 0\n",
      "Epoch [539 / 600] Training loss: 62.427111 Validation Loss: 68.705821 e: 0\n",
      "Epoch [540 / 600] Training loss: 62.530565 Validation Loss: 68.704216 e: 0\n",
      "Epoch [541 / 600] Training loss: 62.706300 Validation Loss: 68.702550 e: 0\n",
      "Epoch [542 / 600] Training loss: 62.627531 Validation Loss: 68.700727 e: 0\n",
      "Epoch [543 / 600] Training loss: 62.483057 Validation Loss: 68.698408 e: 0\n",
      "Epoch [544 / 600] Training loss: 62.519676 Validation Loss: 68.696320 e: 0\n",
      "Epoch [545 / 600] Training loss: 62.429451 Validation Loss: 68.694727 e: 0\n",
      "Epoch [546 / 600] Training loss: 62.537295 Validation Loss: 68.692831 e: 0\n",
      "Epoch [547 / 600] Training loss: 62.502705 Validation Loss: 68.689851 e: 0\n",
      "Epoch [548 / 600] Training loss: 62.583993 Validation Loss: 68.688565 e: 0\n",
      "Epoch [549 / 600] Training loss: 62.499232 Validation Loss: 68.686902 e: 0\n",
      "Epoch [550 / 600] Training loss: 62.538191 Validation Loss: 68.684441 e: 0\n",
      "Epoch [551 / 600] Training loss: 62.526148 Validation Loss: 68.682442 e: 0\n",
      "Epoch [552 / 600] Training loss: 62.453146 Validation Loss: 68.681132 e: 0\n",
      "Epoch [553 / 600] Training loss: 62.546339 Validation Loss: 68.679062 e: 0\n",
      "Epoch [554 / 600] Training loss: 62.481164 Validation Loss: 68.676356 e: 0\n",
      "Epoch [555 / 600] Training loss: 62.480389 Validation Loss: 68.674400 e: 0\n",
      "Epoch [556 / 600] Training loss: 62.542174 Validation Loss: 68.672141 e: 0\n",
      "Epoch [557 / 600] Training loss: 62.470522 Validation Loss: 68.670734 e: 0\n",
      "Epoch [558 / 600] Training loss: 62.374837 Validation Loss: 68.669108 e: 0\n",
      "Epoch [559 / 600] Training loss: 62.586963 Validation Loss: 68.666882 e: 0\n",
      "Epoch [560 / 600] Training loss: 62.606424 Validation Loss: 68.664508 e: 0\n",
      "Epoch [561 / 600] Training loss: 62.455766 Validation Loss: 68.662650 e: 0\n",
      "Epoch [562 / 600] Training loss: 62.608640 Validation Loss: 68.661270 e: 0\n",
      "Epoch [563 / 600] Training loss: 62.499392 Validation Loss: 68.659969 e: 0\n",
      "Epoch [564 / 600] Training loss: 62.451668 Validation Loss: 68.657406 e: 0\n",
      "Epoch [565 / 600] Training loss: 62.516427 Validation Loss: 68.654792 e: 0\n",
      "Epoch [566 / 600] Training loss: 62.560999 Validation Loss: 68.653373 e: 0\n",
      "Epoch [567 / 600] Training loss: 62.542310 Validation Loss: 68.651648 e: 0\n",
      "Epoch [568 / 600] Training loss: 62.515632 Validation Loss: 68.649446 e: 0\n",
      "Epoch [569 / 600] Training loss: 62.527755 Validation Loss: 68.646909 e: 0\n",
      "Epoch [570 / 600] Training loss: 62.560398 Validation Loss: 68.644951 e: 0\n",
      "Epoch [571 / 600] Training loss: 62.442250 Validation Loss: 68.642923 e: 0\n",
      "Epoch [572 / 600] Training loss: 62.424067 Validation Loss: 68.640890 e: 0\n",
      "Epoch [573 / 600] Training loss: 62.453050 Validation Loss: 68.638159 e: 0\n",
      "Epoch [574 / 600] Training loss: 62.546673 Validation Loss: 68.636354 e: 0\n",
      "Epoch [575 / 600] Training loss: 62.474020 Validation Loss: 68.634206 e: 0\n",
      "Epoch [576 / 600] Training loss: 62.400349 Validation Loss: 68.631776 e: 0\n",
      "Epoch [577 / 600] Training loss: 62.485253 Validation Loss: 68.629917 e: 0\n",
      "Epoch [578 / 600] Training loss: 62.511386 Validation Loss: 68.627086 e: 0\n",
      "Epoch [579 / 600] Training loss: 62.464379 Validation Loss: 68.624929 e: 0\n",
      "Epoch [580 / 600] Training loss: 62.467199 Validation Loss: 68.623009 e: 0\n",
      "Epoch [581 / 600] Training loss: 62.504777 Validation Loss: 68.620843 e: 0\n",
      "Epoch [582 / 600] Training loss: 62.470095 Validation Loss: 68.618063 e: 0\n",
      "Epoch [583 / 600] Training loss: 62.535912 Validation Loss: 68.616255 e: 0\n",
      "Epoch [584 / 600] Training loss: 62.466990 Validation Loss: 68.614122 e: 0\n",
      "Epoch [585 / 600] Training loss: 62.506474 Validation Loss: 68.612021 e: 0\n",
      "Epoch [586 / 600] Training loss: 62.382166 Validation Loss: 68.609840 e: 0\n",
      "Epoch [587 / 600] Training loss: 62.519554 Validation Loss: 68.606807 e: 0\n",
      "Epoch [588 / 600] Training loss: 62.527772 Validation Loss: 68.604580 e: 0\n",
      "Epoch [589 / 600] Training loss: 62.509592 Validation Loss: 68.603215 e: 0\n",
      "Epoch [590 / 600] Training loss: 62.470515 Validation Loss: 68.600310 e: 0\n",
      "Epoch [591 / 600] Training loss: 62.474106 Validation Loss: 68.598159 e: 0\n",
      "Epoch [592 / 600] Training loss: 62.580473 Validation Loss: 68.595998 e: 0\n",
      "Epoch [593 / 600] Training loss: 62.530328 Validation Loss: 68.594209 e: 0\n",
      "Epoch [594 / 600] Training loss: 62.502515 Validation Loss: 68.591552 e: 0\n",
      "Epoch [595 / 600] Training loss: 62.442093 Validation Loss: 68.589284 e: 0\n",
      "Epoch [596 / 600] Training loss: 62.475987 Validation Loss: 68.587268 e: 0\n",
      "Epoch [597 / 600] Training loss: 62.544109 Validation Loss: 68.584740 e: 0\n",
      "Epoch [598 / 600] Training loss: 62.451724 Validation Loss: 68.582328 e: 0\n",
      "Epoch [599 / 600] Training loss: 62.398636 Validation Loss: 68.580202 e: 0\n",
      "Epoch [600 / 600] Training loss: 62.489504 Validation Loss: 68.578317 e: 0\n",
      "Best model epoch:  599\n"
     ]
    }
   ],
   "source": [
    "max_num_epochs = 600 # 150\n",
    "early_stopping_epochs = 30\n",
    "warmup = 30\n",
    "\n",
    "best_model = model\n",
    "best_loss = np.inf\n",
    "e = 0\n",
    "\n",
    "for epoch in range(max_num_epochs):\n",
    "    model.train()\n",
    "    train_loss_avg = []\n",
    "    for x in train_loader:\n",
    "        opt.zero_grad()\n",
    "        loss = -model(x.to(device)).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss_avg.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss_avg = []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_loader:\n",
    "            loss = -model(x.to(device)).mean()\n",
    "            valid_loss_avg.append(loss.item())\n",
    "    val_loss_epoch = np.mean(valid_loss_avg)\n",
    "    \n",
    "    # early-stopping\n",
    "    if val_loss_epoch < best_loss:\n",
    "        e = 0\n",
    "        best_loss = val_loss_epoch\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "    else:\n",
    "        e += 1\n",
    "        if epoch < warmup:\n",
    "            e = 0\n",
    "        if e > early_stopping_epochs:\n",
    "            break\n",
    "\n",
    "    print('Epoch [%d / %d] Training loss: %f Validation Loss: %f e: %d' % \n",
    "          (epoch + 1, max_num_epochs, np.mean(train_loss_avg), val_loss_epoch, e))\n",
    "\n",
    "print('Best model epoch: ', best_model_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1249b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c727ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 37/37 [00:00<00:00, 551.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test LL: -61.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if you use a high number number of bins then you may want to decrease the batch size\n",
    "test_loader = DataLoader(test, batch_size=16, drop_last=False)\n",
    "\n",
    "test_ll = []\n",
    "model.eval()\n",
    "for x in tqdm(test_loader):\n",
    "    test_ll.extend(list(model(x.to(device)).detach().cpu().numpy()))\n",
    "assert len(test_ll) == test.shape[0]\n",
    "print('Test LL: %.2f' % np.mean(test_ll))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
