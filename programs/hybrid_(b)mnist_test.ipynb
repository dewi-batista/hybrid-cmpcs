{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e96322",
   "metadata": {},
   "source": [
    "# Directory shenanigans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11bf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. repo_dir used later\n",
    "repo_dir = os.path.abspath(os.path.join(os.path.abspath(\"\"), \"..\"))\n",
    "\n",
    "# 2. sys.path must be appended for importing modules\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# 3. fix current working directory\n",
    "os.chdir(os.path.abspath(os.path.join(os.path.abspath(\"\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474cee",
   "metadata": {},
   "source": [
    "# Import libaries + assign device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7782371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cm_hybrid import ContinuousMixture\n",
    "from models.lo_hybrid import bins_lo, fast_bins_lo\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from utils.reproducibility import seed_everything\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# assign device (cpu or gpu, if present)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc99b81",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b6eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose True for MNIST and False for binary MNIST\n",
    "use_mnist = False\n",
    "\n",
    "# create data directory (if not done already)\n",
    "data_dir = \"../data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# download MNIST into data directory (if not done already)\n",
    "mnist_test = datasets.MNIST(root=\"../data\", train=False, download=True)\n",
    "labels_mnist_test = mnist_test.targets\n",
    "\n",
    "# convert dataset to tensor\n",
    "mnist_test = mnist_test.data.view(10_000, 784).float()\n",
    "\n",
    "# define test set and binarise if use_mnist is False\n",
    "X_test = mnist_test if use_mnist else (mnist_test / 255 >= 0.5).float()\n",
    "y_test = labels_mnist_test\n",
    "\n",
    "# load val and test sets into dataloaders\n",
    "batch_size = 128\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86133c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstruct_image(grayscale_vector,\n",
    "#                       filename,\n",
    "#                       missing_frac=0.3,\n",
    "#                       missing_color='lightcoral'):\n",
    "#     # → 1D numpy array of length 784\n",
    "#     if isinstance(grayscale_vector, torch.Tensor):\n",
    "#         vec = grayscale_vector.detach().cpu().numpy().reshape(-1)\n",
    "#     else:\n",
    "#         vec = np.asarray(grayscale_vector).reshape(-1)\n",
    "\n",
    "#     # Mask 30% of the first 780 pixels at random\n",
    "#     num_pixels = 780\n",
    "#     num_missing = int(num_pixels * missing_frac)\n",
    "#     missing_idx = np.random.choice(num_pixels, num_missing, replace=False)\n",
    "#     mask = np.zeros_like(vec, dtype=bool)\n",
    "#     mask[missing_idx] = True\n",
    "\n",
    "#     # Build masked array and reshape back to 28×28\n",
    "#     masked_vec = np.ma.array(vec, mask=mask)\n",
    "#     img = masked_vec.reshape((28, 28))\n",
    "\n",
    "#     # Plot with no margins and custom “bad‐pixel” colour\n",
    "#     fig = plt.figure(frameon=False)\n",
    "#     fig.patch.set_visible(False)\n",
    "#     ax = fig.add_axes([0, 0, 1, 1], frameon=False, xticks=[], yticks=[])\n",
    "#     cmap = plt.cm.gray\n",
    "#     cmap.set_bad(color=missing_color)               # colour for missing pixels\n",
    "#     ax.imshow(img, cmap=cmap, interpolation='nearest', aspect='equal')\n",
    "#     ax.axis('off')\n",
    "#     plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "#     # Save tightly‐cropped PDF\n",
    "#     fig.savefig(\n",
    "#         filename,\n",
    "#         bbox_inches='tight',\n",
    "#         pad_inches=0,\n",
    "#         transparent=True\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage on X_test[6]\n",
    "# # (This will overwrite the last 4 pixels if desired, then mask 30% of the first 780.)\n",
    "# # X_test[6][-4:] = torch.tensor([0, 1, 1, 1])\n",
    "# reconstruct_image(\n",
    "#     X_test[6],\n",
    "#     '../../bmnist_sample_3_missing.pdf',\n",
    "#     missing_frac=0.3,\n",
    "#     missing_color='gray'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982315a",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5d2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/s3313093/cm-tpm-main/logs/bmnist/hybrid/latent_dim_16/num_bins_8192/lambda_0.40/version_0/checkpoints/best_model_valid-epoch=66.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/s3313093/venvs/my_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:208\n",
      "\tUserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n"
     ]
    }
   ],
   "source": [
    "# model hyperparams\n",
    "dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "lamda = 0.4\n",
    "latent_dim = 16\n",
    "num_bins_trained = 8_192\n",
    "version_num = 0\n",
    "\n",
    "# load model\n",
    "model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "model.n_chunks = 32\n",
    "model.missing = False # this was True before, check if difference (hope not)\n",
    "model.eval(); # semi-colon to prevent printing model architecture\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbd501",
   "metadata": {},
   "source": [
    "## Evaluate mean negative log-likelihood and classification accuracies (on full and missing data) over the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbd0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    lower_power_bound,\n",
    "    upper_power_bound,\n",
    "    latent_opt,\n",
    "    use_mnist,\n",
    "    missing_rate,\n",
    "    batch_size=512,\n",
    "    seed=42,\n",
    "):\n",
    "    accuracies = []\n",
    "    n_bins_list = [2 ** k for k in range(lower_power_bound, upper_power_bound)]\n",
    "\n",
    "    seed_everything(seed)\n",
    "    test_lls = []\n",
    "    for n_bins in n_bins_list:\n",
    "        model.sampler.n_bins = n_bins\n",
    "        if latent_opt:\n",
    "            z, log_w = bins_lo(model, n_bins, train_loader, valid_loader, max_epochs=20, lr=1e-3, patience=5, device=device)\n",
    "        else:\n",
    "            z, log_w = model.sampler(seed=seed)\n",
    "\n",
    "        all_ll = torch.zeros(len(X_test), 10)\n",
    "\n",
    "        if missing_rate == 0.0:\n",
    "            # If no missingness, we can evaluate log‐lik directly\n",
    "            test_lls.append(-model.eval_loader(test_loader, z, log_w, device=device).mean().item())\n",
    "\n",
    "        # ————— prepare a single mask for all digits —————\n",
    "        if missing_rate != 0.0:\n",
    "            model.missing = True\n",
    "            # draw one mask of the same shape as X_test\n",
    "            # so that the same entries are \"missing\" under every digit‐label hypothesis\n",
    "            mask = torch.rand_like(X_test) < missing_rate\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        for digit in range(10):\n",
    "            # start from clean copy of X_test each time\n",
    "            Xd = X_test.clone()\n",
    "\n",
    "            # apply the pre–computed mask (if any) so it is identical for all digit loops\n",
    "            if mask is not None:\n",
    "                Xd[mask] = float('nan')\n",
    "\n",
    "            # overwrite label channels with this candidate digit\n",
    "            if use_mnist:\n",
    "                Xd[:, -1] = digit\n",
    "            else:\n",
    "                bitstring = [int(b) for b in bin(digit)[2:].zfill(4)]\n",
    "                Xd[:, -4:] = torch.tensor(bitstring, dtype=torch.float)\n",
    "\n",
    "            loader = DataLoader(Xd, batch_size=batch_size, shuffle=False)\n",
    "            ll_chunks = []\n",
    "            with torch.no_grad():\n",
    "                for xb in loader:\n",
    "                    xb = xb.to(device)\n",
    "                    llb = model.forward(xb, z, log_w, k=None, seed=seed)\n",
    "                    ll_chunks.append(llb.cpu())\n",
    "            all_ll[:, digit] = torch.cat(ll_chunks, dim=0)\n",
    "\n",
    "        # classification by maximum log‐likelihood\n",
    "        preds = all_ll.argmax(dim=1)\n",
    "        acc = (preds == y_test.squeeze()).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    if latent_opt:\n",
    "        return n_bins_list, accuracies, z\n",
    "\n",
    "    return n_bins_list, accuracies, test_lls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c3613",
   "metadata": {},
   "source": [
    "## Plot accuracies over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3548f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim_16/n_bins_8192/epoch_66 - LAMBDA 0.40\n",
      "Mean NLL and Accuracy for   256 components: 152.1959 : 0.8380 : 0.8185 : 0.7896 : 0.7441 : 0.6819 : 0.6106 : 0.4040 : 0.2865 : 0.1547 : 0.1183\n",
      "Mean NLL and Accuracy for   512 components: 145.2072 : 0.9103 : 0.8940 : 0.8641 : 0.8178 : 0.7615 : 0.6865 : 0.4411 : 0.2730 : 0.1470 : 0.1180\n",
      "Mean NLL and Accuracy for  1024 components: 139.0824 : 0.9460 : 0.9340 : 0.9116 : 0.8786 : 0.8192 : 0.7401 : 0.4747 : 0.2834 : 0.1471 : 0.1167\n",
      "Mean NLL and Accuracy for  2048 components: 134.0094 : 0.9602 : 0.9497 : 0.9332 : 0.9039 : 0.8603 : 0.7884 : 0.4929 : 0.2722 : 0.1286 : 0.1140\n",
      "Mean NLL and Accuracy for  4096 components: 130.5627 : 0.9678 : 0.9609 : 0.9458 : 0.9226 : 0.8879 : 0.8243 : 0.5486 : 0.3132 : 0.1370 : 0.1159\n",
      "Mean NLL and Accuracy for  8192 components: 127.6564 : 0.9712 : 0.9657 : 0.9521 : 0.9333 : 0.9007 : 0.8409 : 0.5645 : 0.3340 : 0.1472 : 0.1201\n",
      "Mean NLL and Accuracy for 16384 components: 125.3976 : 0.9734 : 0.9675 : 0.9561 : 0.9369 : 0.9082 : 0.8513 : 0.5983 : 0.3642 : 0.1530 : 0.1174\n"
     ]
    }
   ],
   "source": [
    "missing_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 0.95]\n",
    "acc_dict = {k: [] for k in missing_rates}\n",
    "test_lls = []\n",
    "\n",
    "for missing_rate in missing_rates:\n",
    "    n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lower_power_bound=8,\n",
    "        upper_power_bound=15, # reduce this to 9 to speed up debugging\n",
    "        latent_opt=False,\n",
    "        use_mnist=use_mnist,\n",
    "        missing_rate=missing_rate,\n",
    "    )\n",
    "    if missing_rate == 0.0:\n",
    "        test_lls = test_lls_list\n",
    "    acc_dict[missing_rate] = acc_list\n",
    "\n",
    "print(f\"latent_dim_{latent_dim}/n_bins_{num_bins_trained}/epoch_{int(model_path.split('epoch=')[-1].split('.', 1)[0])} - LAMBDA {lamda:.2f}\")\n",
    "for i in range(len(n_bins_list)):\n",
    "    n_bins = n_bins_list[i]\n",
    "    str_to_print = f\"Accuracy for {n_bins:5d} components: {test_lls[i]:8.4f}\"\n",
    "    for missing_rate in missing_rates:\n",
    "        str_to_print += f\" : {acc_dict[missing_rate][i]:.4f}\"\n",
    "    print(str_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ded86",
   "metadata": {},
   "source": [
    "# Monte Carlo estimator bars for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968840cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "\n",
    "# missing_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "# acc_dict = {k: [] for k in missing_rates}\n",
    "# num_runs = 50 # each run takes ~30s w/ N=2^9 and ~?s w/ N=2^14\n",
    "\n",
    "# for run in range(num_runs):\n",
    "#     rand_seed = randint(1, 2 ** 32 - 2)\n",
    "#     for missing_rate in missing_rates:\n",
    "#         n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "#             model=model,\n",
    "#             X_test=X_test,\n",
    "#             y_test=y_test,\n",
    "#             lower_power_bound=8,\n",
    "#             upper_power_bound=9,\n",
    "#             latent_opt=False,\n",
    "#             use_mnist=use_mnist,\n",
    "#             missing_rate=missing_rate,\n",
    "#             seed=rand_seed,\n",
    "#         )\n",
    "#         acc_dict[missing_rate].append(acc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # missing percentages\n",
    "# missing_rates = sorted(acc_dict)\n",
    "# missing_pct = [m * 100 for m in missing_rates]\n",
    "\n",
    "# # compute mean and std (%) for each missing rate\n",
    "# means_pct = np.array([np.mean(acc_dict[m]) * 100 for m in missing_rates])\n",
    "# stds_pct  = np.array([np.std(acc_dict[m], ddof=1) * 100 for m in missing_rates])\n",
    "# print(means_pct, stds_pct)\n",
    "\n",
    "# # upper and lower bounds\n",
    "# upper = means_pct + stds_pct\n",
    "# lower = means_pct - stds_pct\n",
    "\n",
    "# # just plotting things, not too important to understand\n",
    "# base_color = plt.cm.viridis(0.5)\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.fill_between(missing_pct, lower, upper, color=base_color, alpha=0.2) # shade region between the lower and upper lines\n",
    "# plt.plot(missing_pct, means_pct, color=base_color, linestyle='-', marker='o', markersize=4, linewidth=2.5, label='Mean accuracy') # mean line\n",
    "# plt.plot(missing_pct, upper, color=base_color, linestyle='--', linewidth=1) # upper line\n",
    "# plt.plot(missing_pct, lower, color=base_color, linestyle='--', linewidth=1) # lower line\n",
    "# plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "# plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "# plt.xticks(missing_pct, fontsize=9)\n",
    "# ax = plt.gca()\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# for spine in ['left', 'bottom']:\n",
    "#     ax.spines[spine].set_color('#888888')\n",
    "#     ax.spines[spine].set_linewidth(0.8)\n",
    "# y_min = lower.min()\n",
    "# y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "# ax.set_yticks(np.arange(y_min_tick, 101, 10))\n",
    "# ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "# ax.set_ylim(y_min, 100)\n",
    "# plt.yticks(fontsize=9)\n",
    "# plt.legend(loc='upper right', bbox_to_anchor=(1, 1.0085), frameon=True, edgecolor='#888888', facecolor='white', fontsize=8)\n",
    "# plt.tight_layout(pad=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20d33d",
   "metadata": {},
   "source": [
    "# Display just NLLs (this is to speed up obtaining them if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e782cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_bins_list = [2 ** k for k in range(8, 15)]\n",
    "# seed=42\n",
    "# seed_everything(seed=seed)\n",
    "\n",
    "# # model hyperparams\n",
    "# dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "# latent_dim = 32\n",
    "# num_bins_trained = 8_192\n",
    "# version_num = 0\n",
    "# for lamda in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    \n",
    "#     # load model\n",
    "#     model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "#     model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "#     model.n_chunks = 32\n",
    "#     model.missing = False # this was True before, check if difference (hope not)\n",
    "#     model.eval(); # semi-colon to prevent printing model architecture\n",
    "\n",
    "#     out = \"\"\n",
    "#     for n_bins in n_bins_list:\n",
    "#         model.sampler.n_bins = n_bins\n",
    "#         z, log_w = model.sampler(seed=seed)\n",
    "#         test_ll = -model.eval_loader(test_loader, z, log_w, device=device).mean().item()\n",
    "#         # print(f\"Mean NLL for {n_bins:5d} components: {test_ll:6.2f}\")\n",
    "#         out += f\"{test_ll:.2f} & \"\n",
    "#     print(\"\\hline\")\n",
    "#     print(f\"{lamda:.1f} & \" + out[:-3] + 2 * \"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030609a",
   "metadata": {},
   "source": [
    "# Plot accuracies for fixed lamda and varying number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to percentages\n",
    "# missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "# perf_matrix_pct = (np.array([acc_dict[m] for m in missing_rates]) * 100).T\n",
    "\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# for idx, temp in enumerate(n_bins_list):\n",
    "#     plt.plot(\n",
    "#         missing_rates_pct,\n",
    "#         perf_matrix_pct[idx],\n",
    "#         linestyle='--',      # dashed lines\n",
    "#         marker='o',\n",
    "#         markersize=4,\n",
    "#         linewidth=2.5,       # a bit thicker\n",
    "#         label=str(temp),\n",
    "#     )\n",
    "\n",
    "# # Axis labels\n",
    "# plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "# plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# # X‐ticks\n",
    "# plt.xticks(missing_rates_pct, fontsize=9)\n",
    "\n",
    "# ax = plt.gca()\n",
    "# # Remove top/right spines\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# # Lighten left/bottom spines\n",
    "# for spine in ['left', 'bottom']:\n",
    "#     ax.spines[spine].set_color('#888888')\n",
    "#     ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# # Dynamic y‐axis: from min accuracy up to 100%\n",
    "# y_min = perf_matrix_pct.min()\n",
    "# y_min_tick = int(np.floor(y_min / 10) * 10)   # round down to nearest 10\n",
    "# ax.set_yticks(np.arange(y_min_tick, 101, 10))  # ticks every 10%\n",
    "# # Make horizontal grid lines more prominent\n",
    "# ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "# ax.set_ylim(y_min, 100)\n",
    "# plt.yticks(fontsize=9)\n",
    "\n",
    "# # Legend in top right with a box, shifted up slightly\n",
    "# plt.legend(\n",
    "#     loc='upper right',\n",
    "#     bbox_to_anchor=(1, 1.0085),  # move legend a bit higher\n",
    "#     frameon=True,\n",
    "#     edgecolor='#888888',\n",
    "#     facecolor='white',\n",
    "#     fontsize=8\n",
    "# )\n",
    "\n",
    "# plt.tight_layout(pad=0.5)\n",
    "# plt.savefig(f\"../figures/lam_{lamda}_acc.pdf\")\n",
    "# plt.savefig(f\"../figures/lam_{lamda}_acc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380be9dc",
   "metadata": {},
   "source": [
    "# Plot accuracies for fixed number of bins and varying lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5384d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamdas = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "# missing_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "# acc_dict = {k: [] for k in lamdas}\n",
    "# for lamda in lamdas:\n",
    "    \n",
    "#     # model hyperparams\n",
    "#     dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "#     latent_dim = 32\n",
    "#     num_bins_trained = 8_192\n",
    "#     version_num = 0\n",
    "\n",
    "#     # load model\n",
    "#     model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "#     model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "#     # model.sampler.n_bins = 16_384 # this is done already in the compute_acc() function, unneeded\n",
    "#     model.n_chunks = 32\n",
    "#     model.missing = False\n",
    "#     model.eval(); # semi-colon to prevent printing model architecture\n",
    "\n",
    "#     for missing_rate in missing_rates:\n",
    "#         n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "#             model=model,\n",
    "#             X_test=X_test,\n",
    "#             y_test=y_test,\n",
    "#             lower_power_bound=14,\n",
    "#             upper_power_bound=15,\n",
    "#             latent_opt=False,\n",
    "#             use_mnist=use_mnist,\n",
    "#             missing_rate=missing_rate,\n",
    "#         )\n",
    "#         acc_dict[lamda].append(acc_list[0])\n",
    "\n",
    "# print(f\"latent_dim_{latent_dim}/n_bins_{num_bins_trained}\")\n",
    "# for lam in lamdas:\n",
    "#     str_to_print = f\"Accuracy for {lam:.1f}\"\n",
    "#     for i in range(len(missing_rates)):\n",
    "#         str_to_print += f\" : {acc_dict[lam][i]:.4f}\"\n",
    "#     print(str_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afe7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert missing rates to percentages\n",
    "# missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "\n",
    "# # Convert each λ’s accuracy‐list to percentages\n",
    "# lambda_dict_pct = {lam: (np.array(acc_dict[lam]) * 100) for lam in acc_dict}\n",
    "\n",
    "# # Sort λ’s so the legend always reads in ascending order\n",
    "# sorted_lambdas = sorted(lambda_dict_pct.keys())\n",
    "\n",
    "# # Sample one color per λ from a colormap\n",
    "# cmap = plt.cm.viridis\n",
    "# colors = cmap(np.linspace(0, 1, len(sorted_lambdas)))\n",
    "\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# for idx, lam in enumerate(sorted_lambdas):\n",
    "#     acc_pct = lambda_dict_pct[lam]\n",
    "#     plt.plot(\n",
    "#         missing_rates_pct,\n",
    "#         acc_pct,\n",
    "#         # color=colors[idx],\n",
    "#         linestyle='--',\n",
    "#         marker='o',\n",
    "#         markersize=4,\n",
    "#         linewidth=2.5,\n",
    "#         label=f\"λ = {lam}\",\n",
    "#     )\n",
    "\n",
    "# # Axis labels\n",
    "# plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "# plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# # X‐ticks\n",
    "# plt.xticks(missing_rates_pct, fontsize=9)\n",
    "\n",
    "# ax = plt.gca()\n",
    "# # Remove top/right spines\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# # Lighten left/bottom spines\n",
    "# for spine in ['left', 'bottom']:\n",
    "#     ax.spines[spine].set_color('#888888')\n",
    "#     ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# # Dynamic y‐axis: from min accuracy up to 100%\n",
    "# all_acc_values = np.concatenate([lambda_dict_pct[lam] for lam in sorted_lambdas])\n",
    "# y_min = all_acc_values.min()\n",
    "# y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "# ax.set_yticks(np.arange(y_min_tick, 101, 10))\n",
    "# ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "# ax.set_ylim(y_min, 100)\n",
    "# plt.yticks(fontsize=9)\n",
    "\n",
    "# # Legend in top right\n",
    "# plt.legend(\n",
    "#     loc='upper right',\n",
    "#     bbox_to_anchor=(1, 1.0085),\n",
    "#     frameon=True,\n",
    "#     edgecolor='#888888',\n",
    "#     facecolor='white',\n",
    "#     fontsize=8\n",
    "# )\n",
    "\n",
    "# plt.tight_layout(pad=0.5)\n",
    "# plt.savefig(f\"../figures/accuracies/accs_latent_dim_{latent_dim}.pdf\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert missing rates to percentages\n",
    "# missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "\n",
    "# # Convert each λ’s accuracy‐list to percentages\n",
    "# lambda_dict_pct = {\n",
    "#     lam: (np.array(acc_dict[lam]) * 100)\n",
    "#     for lam in acc_dict\n",
    "# }\n",
    "\n",
    "# # Sort λ’s so the legend always reads in ascending order\n",
    "# sorted_lambdas = sorted(lambda_dict_pct.keys())\n",
    "\n",
    "# # Sample one color per λ from a colormap\n",
    "# cmap = plt.cm.viridis\n",
    "# colors = cmap(np.linspace(0, 1, len(sorted_lambdas)))\n",
    "\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# for idx, lam in enumerate(sorted_lambdas):\n",
    "#     acc_pct = lambda_dict_pct[lam]\n",
    "#     plt.plot(\n",
    "#         missing_rates_pct,\n",
    "#         acc_pct,\n",
    "#         # color=colors[idx],\n",
    "#         linestyle='--',\n",
    "#         marker='o',\n",
    "#         markersize=4,\n",
    "#         linewidth=1.0,\n",
    "#         label=f\"λ = {lam}\",\n",
    "#     )\n",
    "\n",
    "# # Axis labels\n",
    "# plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "# plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# # X‐ticks\n",
    "# # plt.xticks(missing_rates_pct, fontsize=9)\n",
    "\n",
    "# x_thresh = 31\n",
    "# ax = plt.gca()\n",
    "# ax.set_xticks(np.arange(0, x_thresh + 1, 10))\n",
    "# ax.set_xlim(0, x_thresh)\n",
    "# plt.xticks(fontsize=9)\n",
    "\n",
    "# ax = plt.gca()\n",
    "# # Remove top/right spines\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# # Lighten left/bottom spines\n",
    "# for spine in ['left', 'bottom']:\n",
    "#     ax.spines[spine].set_color('#888888')\n",
    "#     ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# # Dynamic y‐axis: from min accuracy up to 100%\n",
    "# all_acc_values = np.concatenate([lambda_dict_pct[lam] for lam in sorted_lambdas])\n",
    "# y_min = 89\n",
    "# y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "# ax.set_yticks(np.arange(y_min_tick, 101, 1))\n",
    "# ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "# ax.set_ylim(89.5, 97.5)\n",
    "# plt.yticks(fontsize=9)\n",
    "\n",
    "# # Legend in top right\n",
    "# plt.legend(\n",
    "#     loc='upper right',\n",
    "#     bbox_to_anchor=(1, 1.0085),\n",
    "#     frameon=True,\n",
    "#     edgecolor='#888888',\n",
    "#     facecolor='white',\n",
    "#     fontsize=8\n",
    "# )\n",
    "\n",
    "# plt.tight_layout(pad=0.5)\n",
    "# plt.savefig(f\"../figures/accuracies/accs_latent_dim_{latent_dim}_zoomed.pdf\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72939ad7",
   "metadata": {},
   "source": [
    "## Evaluate sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb02c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "\n",
    "# decoder_arch = 'MLP'\n",
    "# n_bins = 32\n",
    "# model.sampler.n_bins = n_bins\n",
    "# z, log_w = model.sampler(seed=42)\n",
    "# logits_tensor = model.decoder.net(z.to(device))\n",
    "# chosen_idxs = sample(range(n_bins), 9)\n",
    "\n",
    "# # 3 by 3 figure of samples\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "# axes = axes.flatten()\n",
    "# for ax, idx in zip(axes, chosen_idxs):\n",
    "#     logits_sample = logits_tensor[idx]\n",
    "#     if decoder_arch == 'tConv':\n",
    "#         logits_permute = logits_sample.permute(1, 2, 0)\n",
    "#         logits_flat = logits_permute.reshape(-1, logits_permute.shape[-1])\n",
    "#         sampled_pixel_vals = torch.distributions.Categorical(logits=logits_flat).sample()\n",
    "#         img = sampled_pixel_vals.view(28, 28).detach().cpu().numpy()\n",
    "#     else:\n",
    "#         probs = torch.sigmoid(logits_sample)\n",
    "#         sample_flat = torch.bernoulli(probs)\n",
    "#         img = sample_flat.view(28, 28).detach().cpu().numpy()\n",
    "\n",
    "#     ax.imshow(img, cmap='gray')\n",
    "#     ax.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"../figures/samples/latent_dim_{latent_dim}/lam_{lamda:.2f}_latent_dim_{latent_dim}_samples.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d397b55",
   "metadata": {},
   "source": [
    "# Load training and validation (for latent opt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5203a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST into data directory (if needed)\n",
    "mnist_train_and_val = datasets.MNIST(root=\"../data\", train=True, download=True)\n",
    "\n",
    "# assign labels\n",
    "labels_mnist_train_and_val = mnist_train_and_val.targets\n",
    "\n",
    "# convert datasets to tensors\n",
    "mnist_train_and_val = mnist_train_and_val.data.view(60_000, 784).float()\n",
    "\n",
    "# embed class label in final pixel(s) of training samples\n",
    "for idx in range(mnist_train_and_val.shape[0]):\n",
    "    label = labels_mnist_train_and_val[idx]\n",
    "    if use_mnist:\n",
    "        mnist_train_and_val[idx][-1] = label\n",
    "        # bin_label = torch.tensor([int(d) for d in bin(label)[2:].zfill(4)]).float()\n",
    "        # mnist_train_and_val[idx][-4:] = bin_label\n",
    "    else:\n",
    "        binary_label = 255 * torch.tensor([int(d) for d in bin(label)[2:].zfill(4)]).float()\n",
    "        mnist_train_and_val[idx][-4:] = binary_label\n",
    "\n",
    "# define train and validation\n",
    "if use_mnist:\n",
    "    X_train = mnist_train_and_val[0:50_000]\n",
    "    X_val   = mnist_train_and_val[50_000::]\n",
    "else: # if use_mnist is False then binarise\n",
    "    X_train = (mnist_train_and_val[0:50_000] / 255 >= 0.5).float()\n",
    "    X_val   = (mnist_train_and_val[50_000::] / 255 >= 0.5).float()\n",
    "\n",
    "y_train = labels_mnist_train_and_val[0:50_000]\n",
    "y_val   = labels_mnist_train_and_val[50_000::]\n",
    "\n",
    "# load data into data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(X_val  , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79ca70",
   "metadata": {},
   "source": [
    "# Latent optimisation: test on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87479e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: [0.9510999917984009, 0.9635999798774719, 0.9681000113487244, 0.9702000021934509, 0.9697999954223633, 0.9713000059127808, 0.9718999862670898], 0.1: [0.9472000002861023, 0.9567999839782715, 0.9585999846458435, 0.9621999859809875, 0.964900016784668, 0.9661999940872192, 0.9666000008583069], 0.2: [0.9297999739646912, 0.9397000074386597, 0.9474999904632568, 0.9513999819755554, 0.9555000066757202, 0.9541000127792358, 0.9559999704360962], 0.3: [0.8996000289916992, 0.9093000292778015, 0.9214000105857849, 0.9301999807357788, 0.9330999851226807, 0.9351999759674072, 0.9387999773025513], 0.4: [0.8482999801635742, 0.8565999865531921, 0.8747000098228455, 0.8899000287055969, 0.8942000269889832, 0.9045000076293945, 0.9077000021934509], 0.5: [0.7649000287055969, 0.7717000246047974, 0.8025000095367432, 0.8199999928474426, 0.8270000219345093, 0.8476999998092651, 0.8486999869346619], 0.7: [0.5175999999046326, 0.4652999937534332, 0.5346999764442444, 0.5422999858856201, 0.5472000241279602, 0.5623999834060669, 0.5680999755859375], 0.8: [0.34769999980926514, 0.2498999983072281, 0.3474999964237213, 0.34049999713897705, 0.3368000090122223, 0.3296000063419342, 0.3255000114440918], 0.9: [0.20970000326633453, 0.11919999867677689, 0.16259999573230743, 0.1941000074148178, 0.18240000307559967, 0.18299999833106995, 0.16539999842643738], 0.95: [0.1703999936580658, 0.10109999775886536, 0.11779999732971191, 0.14259999990463257, 0.15649999678134918, 0.1679999977350235, 0.14149999618530273]}\n",
      "Accuracy for   256 components : 0.9511 : 0.9472 : 0.9298 : 0.8996 : 0.8483 : 0.7649 : 0.5176 : 0.3477 : 0.2097 : 0.1704\n",
      "Accuracy for   512 components : 0.9636 : 0.9568 : 0.9397 : 0.9093 : 0.8566 : 0.7717 : 0.4653 : 0.2499 : 0.1192 : 0.1011\n",
      "Accuracy for  1024 components : 0.9681 : 0.9586 : 0.9475 : 0.9214 : 0.8747 : 0.8025 : 0.5347 : 0.3475 : 0.1626 : 0.1178\n",
      "Accuracy for  2048 components : 0.9702 : 0.9622 : 0.9514 : 0.9302 : 0.8899 : 0.8200 : 0.5423 : 0.3405 : 0.1941 : 0.1426\n",
      "Accuracy for  4096 components : 0.9698 : 0.9649 : 0.9555 : 0.9331 : 0.8942 : 0.8270 : 0.5472 : 0.3368 : 0.1824 : 0.1565\n",
      "Accuracy for  8192 components : 0.9713 : 0.9662 : 0.9541 : 0.9352 : 0.9045 : 0.8477 : 0.5624 : 0.3296 : 0.1830 : 0.1680\n",
      "Accuracy for 16384 components : 0.9719 : 0.9666 : 0.9560 : 0.9388 : 0.9077 : 0.8487 : 0.5681 : 0.3255 : 0.1654 : 0.1415\n"
     ]
    }
   ],
   "source": [
    "missing_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 0.95]\n",
    "acc_dict = {k: [] for k in missing_rates}\n",
    "test_lls = []\n",
    "\n",
    "for missing_rate in missing_rates:\n",
    "    n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lower_power_bound=8,\n",
    "        upper_power_bound=15, # reduce this to 9 to speed up debugging\n",
    "        latent_opt=True,\n",
    "        use_mnist=use_mnist,\n",
    "        missing_rate=missing_rate,\n",
    "    )\n",
    "    if missing_rate == 0.0:\n",
    "        test_lls = test_lls_list\n",
    "    acc_dict[missing_rate] = acc_list\n",
    "print(acc_dict)\n",
    "\n",
    "for i in range(len(n_bins_list)):\n",
    "    n_bins = n_bins_list[i]\n",
    "    str_to_print = f\"Accuracy for {n_bins:5d} components\"\n",
    "    for missing_rate in missing_rates:\n",
    "        str_to_print += f\" : {acc_dict[missing_rate][i]:.4f}\"\n",
    "    print(str_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_16/n_bins_8192/epoch_66 - LAMBDA 0.40\n",
    "Accuracy for   256 components: 152.1959 : 0.8380 : 0.8185 : 0.7896 : 0.7441 : 0.6819 : 0.6106 : 0.4040 : 0.2865 : 0.1547 : 0.1183\n",
    "Accuracy for   512 components: 145.2072 : 0.9103 : 0.8940 : 0.8641 : 0.8178 : 0.7615 : 0.6865 : 0.4411 : 0.2730 : 0.1470 : 0.1180\n",
    "Accuracy for  1024 components: 139.0824 : 0.9460 : 0.9340 : 0.9116 : 0.8786 : 0.8192 : 0.7401 : 0.4747 : 0.2834 : 0.1471 : 0.1167\n",
    "Accuracy for  2048 components: 134.0094 : 0.9602 : 0.9497 : 0.9332 : 0.9039 : 0.8603 : 0.7884 : 0.4929 : 0.2722 : 0.1286 : 0.1140\n",
    "Accuracy for  4096 components: 130.5627 : 0.9678 : 0.9609 : 0.9458 : 0.9226 : 0.8879 : 0.8243 : 0.5486 : 0.3132 : 0.1370 : 0.1159\n",
    "Accuracy for  8192 components: 127.6564 : 0.9712 : 0.9657 : 0.9521 : 0.9333 : 0.9007 : 0.8409 : 0.5645 : 0.3340 : 0.1472 : 0.1201\n",
    "Accuracy for 16384 components: 125.3976 : 0.9734 : 0.9675 : 0.9561 : 0.9369 : 0.9082 : 0.8513 : 0.5983 : 0.3642 : 0.1530 : 0.1174\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3f33c",
   "metadata": {},
   "source": [
    "## Evaluate sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "decoder_arch = 'MLP'\n",
    "n_bins = 32\n",
    "model.sampler.n_bins = n_bins\n",
    "z, log_w = model.sampler(seed=42)\n",
    "logits_tensor = model.decoder.net(z.to(device))\n",
    "chosen_idxs = sample(range(n_bins), 9)\n",
    "\n",
    "# 3 by 3 figure of samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "axes = axes.flatten()\n",
    "for ax, idx in zip(axes, chosen_idxs):\n",
    "    logits_sample = logits_tensor[idx]\n",
    "    if decoder_arch == 'tConv':\n",
    "        logits_permute = logits_sample.permute(1, 2, 0)\n",
    "        logits_flat = logits_permute.reshape(-1, logits_permute.shape[-1])\n",
    "        sampled_pixel_vals = torch.distributions.Categorical(logits=logits_flat).sample()\n",
    "        img = sampled_pixel_vals.view(28, 28).detach().cpu().numpy()\n",
    "    else:\n",
    "        probs = torch.sigmoid(logits_sample)\n",
    "        sample_flat = torch.bernoulli(probs)\n",
    "        img = sample_flat.view(28, 28).detach().cpu().numpy()\n",
    "\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../figures/samples/latent_dim_{latent_dim}/lam_{lamda:.2f}_latent_dim_{latent_dim}_samples.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
