{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e96322",
   "metadata": {},
   "source": [
    "# Align directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. repo_dir used later\n",
    "repo_dir = os.path.abspath(os.path.join(os.path.abspath(\"\"), \"..\"))\n",
    "\n",
    "# 2. sys.path must be appended for importing modules\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# 3. fix current working directory\n",
    "os.chdir(os.path.abspath(os.path.join(os.path.abspath(\"\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474cee",
   "metadata": {},
   "source": [
    "# Import libaries + assign device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7782371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign discriminative or generative model boolean\n",
    "test_discrim = True\n",
    "model_type = 'discrim' if test_discrim else 'gen'\n",
    "\n",
    "if test_discrim:\n",
    "    import models.cm_discrim as cm\n",
    "    from models.lo_discrim import bins_lo, fast_bins_lo\n",
    "else:\n",
    "    import models.cm_gen as cm\n",
    "    from models.lo_gen import bins_lo, fast_bins_lo\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from utils.reproducibility import seed_everything\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# assign device (cpu or gpu, if present)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc99b81",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose True for MNIST and False for binary MNIST\n",
    "use_mnist = False\n",
    "\n",
    "# create data directory (if not done already)\n",
    "data_dir = \"../data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# download MNIST into data directory (if not done already)\n",
    "mnist_test = datasets.MNIST(root=\"../data\", train=False, download=True)\n",
    "labels_mnist_test = mnist_test.targets\n",
    "\n",
    "# convert dataset to tensor\n",
    "mnist_test = mnist_test.data.view(10_000, 784).float()\n",
    "\n",
    "# define test set and binarise if use_mnist is False\n",
    "X_test = mnist_test if use_mnist else (mnist_test / 255 >= 0.5).float()\n",
    "y_test = labels_mnist_test\n",
    "\n",
    "# load val and test sets into dataloaders\n",
    "batch_size = 128\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982315a",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparams\n",
    "dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "latent_dim = 16\n",
    "num_bins_trained = 2 ** 14\n",
    "version_num = 0\n",
    "\n",
    "# load model\n",
    "decoder_arch = 'tConv'\n",
    "model_path = glob.glob(repo_dir+f'/logs/{dataset}/{model_type}/{decoder_arch}/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/version_{version_num}/checkpoints/*.ckpt')[0] # for bmnist\n",
    "# model_path = glob.glob(repo_dir+f'/logs/{dataset}/{model_type}/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/version_{version_num}/checkpoints/*.ckpt')[0] # for mnist\n",
    "print(model_path)\n",
    "model = cm.ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "model.n_chunks = 32\n",
    "model.missing = False # this was True before, check if difference (hope not)\n",
    "model.eval(); # semi-colon to prevent printing model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b6d8",
   "metadata": {},
   "source": [
    "## Evaluate classification accuracy (on full and missing data) and log-likelihood of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    lower_power_bound,\n",
    "    upper_power_bound,\n",
    "    latent_opt,\n",
    "    use_mnist=True,\n",
    "    missing=False,\n",
    "    missing_rate=0.6,\n",
    "    batch_size_full=512,\n",
    "    batch_size_missing=256,\n",
    "    seed=42,\n",
    "):\n",
    "    accuracies = []\n",
    "    bins_list = [2 ** k for k in range(lower_power_bound, upper_power_bound)]\n",
    "    seed_everything(seed)\n",
    "    test_lls = []\n",
    "    for n_bins in bins_list:\n",
    "        model.sampler.n_bins = n_bins\n",
    "        if latent_opt:\n",
    "            z, log_w = bins_lo(model, n_bins, train_loader, valid_loader, max_epochs=20, lr=1e-3, patience=5, device=device)\n",
    "        else:\n",
    "            z, log_w = model.sampler(seed=seed)\n",
    "        all_ll = torch.zeros(len(X_test), 10)\n",
    "        if not missing:\n",
    "            test_lls.append([n_bins, model.eval_loader(test_loader, z, log_w, device=device).mean().item()])\n",
    "        for digit in range(10):\n",
    "            Xd = X_test.clone()\n",
    "            # if missing mode, randomly mask entries\n",
    "            if missing:\n",
    "                model.missing = True\n",
    "                mask = torch.rand_like(Xd) < missing_rate\n",
    "                Xd[mask] = float('nan')\n",
    "            # overwrite label channels\n",
    "            if use_mnist:\n",
    "                Xd[:, -1] = digit\n",
    "            else:\n",
    "                bits = torch.tensor([\n",
    "                    int(b) for b in bin(digit)[2:].zfill(4)\n",
    "                ], dtype=torch.float)\n",
    "                Xd[:, -4:] = bits\n",
    "\n",
    "            loader = DataLoader(\n",
    "                Xd,\n",
    "                batch_size=(batch_size_missing if missing else batch_size_full),\n",
    "                shuffle=False\n",
    "            )\n",
    "            ll_chunks = []\n",
    "            with torch.no_grad():\n",
    "                for xb in loader:\n",
    "                    xb = xb.to(device)\n",
    "                    llb = model.forward(xb, z, log_w, k=None, seed=seed)\n",
    "                    ll_chunks.append(llb.cpu())\n",
    "            all_ll[:, digit] = torch.cat(ll_chunks, dim=0)\n",
    "\n",
    "        # classification by max log-likelihood\n",
    "        preds = all_ll.argmax(dim=1)\n",
    "        acc = (preds == y_test.squeeze()).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "        line_class = f\"Accuracy for {n_bins:5d} bins: {acc:.4f} : \"\n",
    "        print(line_class)\n",
    "\n",
    "    if not missing:\n",
    "        print()\n",
    "        for (n_bins, test_ll) in test_lls:\n",
    "            print(f\"Test LL for {n_bins:5d} bins: {test_ll:.4f} : \")\n",
    "    print()\n",
    "\n",
    "    if latent_opt:\n",
    "        return bins_list, accuracies, z\n",
    "    \n",
    "    return bins_list, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b4d4b",
   "metadata": {},
   "source": [
    "# Test on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b55372",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_full, acc_full = compute_accuracies(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    lower_power_bound=15,\n",
    "    upper_power_bound=17,\n",
    "    latent_opt=False,\n",
    "    use_mnist=use_mnist,\n",
    "    missing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336fc64",
   "metadata": {},
   "source": [
    "# (TEMP): automate benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparams\n",
    "dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "decoder_arch = 'tConv'\n",
    "latent_dim = 16\n",
    "version_num = 0\n",
    "\n",
    "bins_list = [2 ** k for k in range(8, 14)]\n",
    "for num_bins_trained in bins_list:\n",
    "    model_path = glob.glob(repo_dir+f'/logs/{dataset}/{model_type}/{decoder_arch}/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/version_{version_num}/checkpoints/*.ckpt')[0] # for bmnist\n",
    "    # model_path = glob.glob(repo_dir+f'/logs/{dataset}/{model_type}/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/version_{version_num}/checkpoints/*.ckpt')[0] # for mnist\n",
    "    model = cm.ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "    model.n_chunks = 32\n",
    "    model.missing = False # this was True before, check if difference (hope not)\n",
    "    model.eval(); # semi-colon to prevent printing model architecture\n",
    "\n",
    "    bins_full, acc_full = compute_accuracies(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lower_power_bound=8,\n",
    "        upper_power_bound=12,\n",
    "        latent_opt=False,\n",
    "        use_mnist=use_mnist,\n",
    "        missing=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525bc49",
   "metadata": {},
   "source": [
    "# Test on missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f698b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_missing, acc_missing = compute_accuracies(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    lower_power_bound=8,\n",
    "    upper_power_bound=12,\n",
    "    latent_opt=False,\n",
    "    use_mnist=use_mnist,\n",
    "    missing=True,\n",
    "    missing_rate=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c562d",
   "metadata": {},
   "source": [
    "# Plot both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ordinal positions\n",
    "pos = np.arange(len(bins_full))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot at equalâ€spaced positions\n",
    "plt.plot(pos, [100 * a for a in acc_full], marker='o', linestyle='-', label='Full Data')\n",
    "plt.plot(pos, [100 * a for a in acc_missing], marker='s', linestyle='--', label='Missing Data')\n",
    "\n",
    "# Label each tick by its 2^k value\n",
    "xtick_labels = [f\"$2^{{{int(np.log2(b))}}}$\" for b in bins_full]\n",
    "plt.xticks(pos, xtick_labels)\n",
    "\n",
    "plt.yticks(np.arange(10, 101, 10))\n",
    "plt.xlabel(\"Number of Bins\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy vs. Number of Bins\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928665b",
   "metadata": {},
   "source": [
    "## Evaluate sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(grayscale_vector):\n",
    "    image_data = grayscale_vector.reshape((28, 28))\n",
    "    plt.imshow(image_data, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "from random import randint\n",
    "\n",
    "n_bins = 32\n",
    "model.sampler.n_bins = n_bins\n",
    "z, log_w = model.sampler(seed=42)\n",
    "logits_tensor = model.decoder.net(z.to(device))\n",
    "logits_sample = logits_tensor[randint(0, n_bins - 1)]\n",
    "\n",
    "if decoder_arch == 'tConv':\n",
    "    logits_permute = logits_sample.permute(1, 2, 0)\n",
    "    logits_flat = logits_permute.reshape(-1, logits_permute.shape[-1])\n",
    "    sampled_pixel_vals = torch.distributions.Categorical(logits=logits_flat).sample()\n",
    "    reconstruct_image(sampled_pixel_vals.view(28, 28).detach().cpu())\n",
    "else:\n",
    "    probs = torch.sigmoid(logits_sample)\n",
    "    sample = torch.bernoulli(probs)\n",
    "    reconstruct_image(sample.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd16ce",
   "metadata": {},
   "source": [
    "# Latent optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc23d6f",
   "metadata": {},
   "source": [
    "# Load training and validation (for latent opt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST into data directory (if needed)\n",
    "mnist_train_and_val = datasets.MNIST(root=\"../data\", train=True, download=True)\n",
    "\n",
    "# assign labels\n",
    "labels_mnist_train_and_val = mnist_train_and_val.targets\n",
    "\n",
    "# convert datasets to tensors\n",
    "mnist_train_and_val = mnist_train_and_val.data.view(60_000, 784).float()\n",
    "\n",
    "# embed class label in final pixel(s) of training samples\n",
    "for idx in range(mnist_train_and_val.shape[0]):\n",
    "    label = labels_mnist_train_and_val[idx]\n",
    "    if use_mnist:\n",
    "        mnist_train_and_val[idx][-1] = label\n",
    "        # bin_label = torch.tensor([int(d) for d in bin(label)[2:].zfill(4)]).float()\n",
    "        # mnist_train_and_val[idx][-4:] = bin_label\n",
    "    else:\n",
    "        binary_label = 255 * torch.tensor([int(d) for d in bin(label)[2:].zfill(4)]).float()\n",
    "        mnist_train_and_val[idx][-4:] = binary_label\n",
    "\n",
    "# define train and validation\n",
    "if use_mnist:\n",
    "    X_train = mnist_train_and_val[0:50_000]\n",
    "    X_val   = mnist_train_and_val[50_000::]\n",
    "else: # if use_mnist is False then binarise\n",
    "    X_train = (mnist_train_and_val[0:50_000] / 255 >= 0.5).float()\n",
    "    X_val   = (mnist_train_and_val[50_000::] / 255 >= 0.5).float()\n",
    "\n",
    "y_train = labels_mnist_train_and_val[0:50_000]\n",
    "y_val   = labels_mnist_train_and_val[50_000::]\n",
    "\n",
    "# load data into data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(X_val  , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f5aa5",
   "metadata": {},
   "source": [
    "# Latent optimisation: test on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_path)\n",
    "# latent optimisation should really be done here to avoid doing it again for missing\n",
    "bins_full, acc_full, z_opt = compute_accuracies(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    lower_power_bound=8,\n",
    "    upper_power_bound=9,\n",
    "    latent_opt=True,\n",
    "    use_mnist=use_mnist,\n",
    "    missing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a0699",
   "metadata": {},
   "source": [
    "# Latent optimisation: test on missing data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41412bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_missing, acc_missing, z_opt = compute_accuracies(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    lower_power_bound=8,\n",
    "    upper_power_bound=12,\n",
    "    latent_opt=True,\n",
    "    use_mnist=use_mnist,\n",
    "    missing=True,\n",
    "    missing_rate=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50179e6a",
   "metadata": {},
   "source": [
    "# Evaluate sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd186ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(grayscale_vector):\n",
    "    image_data = grayscale_vector.reshape((28, 28))\n",
    "    plt.imshow(image_data, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "from random import randint\n",
    "\n",
    "n_bins = 32\n",
    "model.sampler.n_bins = n_bins\n",
    "# latent z_opt used here!\n",
    "logits_tensor = model.decoder.net(z_opt.to(device))\n",
    "logits_sample = logits_tensor[randint(0, n_bins - 1)]\n",
    "\n",
    "if decoder_arch == 'tConv':\n",
    "    logits_permute = logits_sample.permute(1, 2, 0)\n",
    "    logits_flat = logits_permute.reshape(-1, logits_permute.shape[-1])\n",
    "    sampled_pixel_vals = torch.distributions.Categorical(logits=logits_flat).sample()\n",
    "    reconstruct_image(sampled_pixel_vals.view(28, 28).detach().cpu())\n",
    "else:\n",
    "    probs = torch.sigmoid(logits_sample)\n",
    "    sample = torch.bernoulli(probs)\n",
    "    reconstruct_image(sample.detach().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
