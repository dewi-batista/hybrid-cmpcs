{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa884554",
   "metadata": {},
   "source": [
    "## Evaluating hybrid CMPCs on Binary MNIST\n",
    "\n",
    "❗ I don't suggest evaluating these models using commerical GPUs unless you are friendly with NVIDIA and have early access to some powerful commercial GPUs.\n",
    "\n",
    "❗ I suggest using a compute cluster in which you can utilise a GPU at least as powerful as a 32 GB V100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e96322",
   "metadata": {},
   "source": [
    "### Directory shenanigans\n",
    "\n",
    "The repo's directory is very possibly dependent on where the workspace is stored on your device. Ideally, running this first block will assign `/cm-tpm-main/` to `repo_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. repo_dir used later\n",
    "repo_dir = os.path.abspath(os.path.join(os.path.abspath(\"\")))\n",
    "\n",
    "# 2. sys.path must be appended for importing modules\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# 3. fix current working directory\n",
    "os.chdir(os.path.abspath(os.path.join(os.path.abspath(\"\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474cee",
   "metadata": {},
   "source": [
    "### Import libaries + assign device (CPU/GPU)\n",
    "\n",
    "It's unlikely that you need to change any of this. Simply run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7782371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cm_hybrid import ContinuousMixture\n",
    "from models.lo_hybrid import bins_lo\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from utils.reproducibility import seed_everything\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# assign device (cpu or gpu, if present)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc99b81",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "It's unlikely that you need to change any of this. Simply run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose True for MNIST and False for Binary MNIST (all benchmarks discussed in the report are for Binary MNIST so it is set to False by default here)\n",
    "use_mnist = False\n",
    "dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "\n",
    "# create data directory (if not done already)\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# download MNIST into data directory (if not done already)\n",
    "mnist_test = datasets.MNIST(root=\"data\", train=False, download=True)\n",
    "labels_mnist_test = mnist_test.targets\n",
    "\n",
    "# convert dataset to tensor\n",
    "mnist_test = mnist_test.data.view(10_000, 784).float()\n",
    "\n",
    "# define test set and binarise if use_mnist is False\n",
    "X_test = mnist_test if use_mnist else (mnist_test / 255 >= 0.5).float()\n",
    "y_test = labels_mnist_test\n",
    "\n",
    "# load val and test sets into dataloaders\n",
    "batch_size = 128\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982315a",
   "metadata": {},
   "source": [
    "### Load model\n",
    "\n",
    "Set the hyperparameters according to which model you would like to evaluate. To see which are available have a look in ```/logs/bmnist/hybrid```.\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "Perhaps today I feel like benchmarking the hybrid CMPC with latent dimension $d=16$, $N_{\\text{train}}=2^{13}$ and $\\lambda=0.4$.\n",
    "\n",
    "I see that this model is trained and ready to be evaluated as the directory `/logs/bmnist/hybrid/latent_dim_16/num_bins_8192/lambda_0.40` exists.\n",
    "\n",
    "As such, I choose `latent_dim=16`, `num_bins_trained=8_192` and `lambda=0.4` below. I suggest leaving the rest of the block alone and running it. If successful you will see a printed string along the lines of\n",
    "$$\\texttt{/cm-tpm-main/logs/bmnist/hybrid/latent\\_dim\\_16/num\\_bins\\_8192/lambda\\_0.40/version\\_0/checkpoints/best\\_model\\_valid-epoch=66.ckpt}.$$\n",
    "If the `UserWarning` is printed too, ignore it. It has no effect on our evaluation and is simply PyTorch being temperamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparams (change these according to which model you would like to evaluate)\n",
    "latent_dim = 32\n",
    "num_bins_trained = 8_192\n",
    "lamda = 0.0\n",
    "\n",
    "# load model\n",
    "version_num = 0 # don't change this from 0 if you're unaware of what it does\n",
    "model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "model.n_chunks = 32\n",
    "model.missing = False # this was True before, check if difference (hope not)\n",
    "model.eval(); # semi-colon to prevent printing model architecture\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbd501",
   "metadata": {},
   "source": [
    "### Function for computing mean negative log-likelihoods and clasisfication accuracies\n",
    "\n",
    "Best to minimise this block as it takes a lot of vertical space and makes scrolling up and down between blocks inconvenient when not minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    lower_power_bound,\n",
    "    upper_power_bound,\n",
    "    latent_opt,\n",
    "    use_mnist,\n",
    "    missing_rate,\n",
    "    batch_size=512,\n",
    "    seed=42,\n",
    "):\n",
    "    accuracies = []\n",
    "    n_bins_list = [2 ** k for k in range(lower_power_bound, upper_power_bound)]\n",
    "\n",
    "    seed_everything(seed)\n",
    "    test_lls = []\n",
    "    for n_bins in n_bins_list:\n",
    "        model.sampler.n_bins = n_bins\n",
    "        if latent_opt:\n",
    "            z, log_w = bins_lo(model, n_bins, train_loader, valid_loader, max_epochs=20, lr=1e-3, patience=5, device=device)\n",
    "        else:\n",
    "            z, log_w = model.sampler(seed=seed)\n",
    "\n",
    "        all_ll = torch.zeros(len(X_test), 10)\n",
    "\n",
    "        if missing_rate == 0.0:\n",
    "            # if no missingness, we can evaluate log‐likelihood directly\n",
    "            test_lls.append(-model.eval_loader(test_loader, z, log_w, device=device).mean().item())\n",
    "\n",
    "        # ————— prepare a single mask for all digits —————\n",
    "        if missing_rate != 0.0:\n",
    "            model.missing = True\n",
    "            # draw one mask of the same shape as X_test\n",
    "            # so that the same entries are \"missing\" under every digit‐label hypothesis\n",
    "            mask = torch.rand_like(X_test) < missing_rate\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        for digit in range(10):\n",
    "            # start from clean copy of X_test each time\n",
    "            Xd = X_test.clone()\n",
    "\n",
    "            # apply the pre–computed mask (if any) so it is identical for all digit loops\n",
    "            if mask is not None:\n",
    "                Xd[mask] = float('nan')\n",
    "\n",
    "            # overwrite label channels with this candidate digit\n",
    "            if use_mnist:\n",
    "                Xd[:, -1] = digit\n",
    "            else:\n",
    "                bitstring = [int(b) for b in bin(digit)[2:].zfill(4)]\n",
    "                Xd[:, -4:] = torch.tensor(bitstring, dtype=torch.float)\n",
    "\n",
    "            loader = DataLoader(Xd, batch_size=batch_size, shuffle=False)\n",
    "            ll_chunks = []\n",
    "            with torch.no_grad():\n",
    "                for xb in loader:\n",
    "                    xb = xb.to(device)\n",
    "                    llb = model.forward(xb, z, log_w, k=None, seed=seed)\n",
    "                    ll_chunks.append(llb.cpu())\n",
    "            all_ll[:, digit] = torch.cat(ll_chunks, dim=0)\n",
    "\n",
    "        # classification by maximum log‐likelihood\n",
    "        preds = all_ll.argmax(dim=1)\n",
    "        acc = (preds == y_test.squeeze()).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    if latent_opt:\n",
    "        return n_bins_list, accuracies, z\n",
    "\n",
    "    return n_bins_list, accuracies, test_lls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c3613",
   "metadata": {},
   "source": [
    "### Evaluation time! Compute mean negative log-likelihoods and classification accuracies on the test set\n",
    "\n",
    "- The single hybrid CMPC evaluated here is fixed according to whichever model was loaded above\n",
    "- The number of components $N_{\\text{test}}$ varies according to $\\texttt{lower\\_power\\_bound}$ and $\\texttt{upper\\_power\\_bound}$\n",
    "- Classification is performed on the test set for various portions of pixels being missing at random. The portion of pixels missing is dictated by the list $\\texttt{missing\\_rates}$.\n",
    "\n",
    "#### **Example:**\n",
    "If we loaded the model with latent dimension $d=16$, $\\texttt{num\\_bins\\_trained}=2^{13}$ and $\\lambda=0.4$ then the model being evaluated on the test set in this block is precisely the hybrid CMPC trained with $N_{\\text{train}}=2^{13}$, $d=16$ and $\\lambda=0.4$. As stated in the report, the number of components at test time $N_{\\text{test}}$ can be whatever we desire, so in the block below we evaluate our hybrid CMPC at test time with $N_{\\text{test}}\\in\\{2^8,\\dots,2^{14}\\}$ (so 7 models are evaluated). As such, the block outputs the mean negative log-likelihood and classification accuracy of each of our 7 models.\n",
    "\n",
    "Further, we evaluate the classification accuracies of these 7 test time models for varying degrees of missing pixel values in each sample. By default, we perform this evaluation for the following list of portions of missing pixel values: $0\\%, 10\\%, 20\\%,\\dots, 90\\%, 95\\%$ (11 distinct portions in total). As such, for each of our 7 evaluated models, we expect the block to return a single mean negative log-likelihood and 11 classification accuracies. The order of the latter is according to the portions of missing pixel values from $0\\%$ to $95\\%$.\n",
    "\n",
    "❗This block takes about 15 minutes to run on a 40 GB A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "acc_dict = {k: [] for k in missing_rates}\n",
    "test_lls = []\n",
    "\n",
    "for missing_rate in missing_rates:\n",
    "    n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        lower_power_bound=8,\n",
    "        upper_power_bound=15, # reduce this to 9 to speed up debugging\n",
    "        latent_opt=False,\n",
    "        use_mnist=use_mnist,\n",
    "        missing_rate=missing_rate,\n",
    "    )\n",
    "    if missing_rate == 0.0:\n",
    "        test_lls = test_lls_list\n",
    "    acc_dict[missing_rate] = acc_list\n",
    "\n",
    "print(f\"latent_dim_{latent_dim}/n_bins_{num_bins_trained}/epoch_{int(model_path.split('epoch=')[-1].split('.', 1)[0])} - LAMBDA {lamda:.2f}\")\n",
    "for i in range(len(n_bins_list)):\n",
    "    n_bins = n_bins_list[i]\n",
    "    str_to_print = f\"Accuracy for {n_bins:5d} components: {test_lls[i]:8.4f}\"\n",
    "    for missing_rate in missing_rates:\n",
    "        str_to_print += f\" : {acc_dict[missing_rate][i]:.4f}\"\n",
    "    print(str_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030609a",
   "metadata": {},
   "source": [
    "### Plot classification accuracies from block above (one curve for each $N_{\\text{test}}\\in\\{2^8,\\dots,2^{14}\\}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert missing rates to percentages\n",
    "missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "perf_matrix_pct = (np.array([acc_dict[m] for m in missing_rates]) * 100).T\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for idx, temp in enumerate(n_bins_list):\n",
    "    plt.plot(\n",
    "        missing_rates_pct,\n",
    "        perf_matrix_pct[idx],\n",
    "        linestyle='--',\n",
    "        marker='o',\n",
    "        markersize=4,\n",
    "        linewidth=2.5,\n",
    "        label=str(temp),\n",
    "    )\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# x‐ticks\n",
    "plt.xticks(missing_rates_pct, fontsize=9)\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "for spine in ['left', 'bottom']:\n",
    "    ax.spines[spine].set_color('#888888')\n",
    "    ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# dynamically changed y‐axis: from min accuracy up to 100%\n",
    "y_min = perf_matrix_pct.min()\n",
    "y_min_tick = int(np.floor(y_min / 10) * 10) # round down to nearest 10\n",
    "ax.set_yticks(np.arange(y_min_tick, 101, 10)) # ticks every 10%\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "ax.set_ylim(y_min, 100)\n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "# force legend to appear in top right\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.0085), frameon=True, edgecolor='#888888', facecolor='white', fontsize=8)\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "plt.savefig(f\"figures/accuracies/lambda_fixed/accs_latent_dim_{latent_dim}_lam_{lamda}.png\")\n",
    "plt.savefig(f\"figures/accuracies/lambda_fixed/accs_latent_dim_{latent_dim}_lam_{lamda}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380be9dc",
   "metadata": {},
   "source": [
    "### Plot classification accuracies for fixed number of components and varying lambdas\n",
    "\n",
    "Here, for $\\lambda\\in\\{0,0.2,0.4,0.6,0.8,1\\}$, $N_{\\text{test}}=2^{14}$ and whichever latent dimension $d\\in\\{2,4,8,16,32\\}$ you desire. For the fixed latent dimension chosen, this block computes the classification accuracies for 6 models: one for each value of $\\lambda$. Again, classification is performed for the test set with various portions of missing pixel values.\n",
    "\n",
    "To change the latent dimension, set `latent_dim` to $2,4,8,16$ or $32$.\n",
    "\n",
    "❗This block takes about 40 minutes to run on a 40 GB A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5384d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparams\n",
    "latent_dim = 32\n",
    "num_bins_trained = 8_192\n",
    "version_num = 0\n",
    "\n",
    "lamdas = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "missing_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "acc_dict = {k: [] for k in lamdas}\n",
    "for lamda in lamdas:\n",
    "\n",
    "    # load model\n",
    "    model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "    model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "    model.n_chunks = 32\n",
    "    model.missing = False\n",
    "    model.eval(); # semi-colon to prevent printing model architecture\n",
    "\n",
    "    for missing_rate in missing_rates:\n",
    "        n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "            model=model,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            lower_power_bound=14,\n",
    "            upper_power_bound=15,\n",
    "            latent_opt=False,\n",
    "            use_mnist=use_mnist,\n",
    "            missing_rate=missing_rate,\n",
    "        )\n",
    "        acc_dict[lamda].append(acc_list[0])\n",
    "\n",
    "print(f\"latent_dim_{latent_dim}/n_bins_{num_bins_trained}\")\n",
    "for lam in lamdas:\n",
    "    str_to_print = f\"Accuracy for {lam:.1f}\"\n",
    "    for i in range(len(missing_rates)):\n",
    "        str_to_print += f\" : {acc_dict[lam][i]:.4f}\"\n",
    "    print(str_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cd8c3",
   "metadata": {},
   "source": [
    "### Plot classification accuracies from block above (one curve for each $\\lambda\\in\\{0,0.2,0.4,0.6,0.8,1\\}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afe7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert missing rates to percentages\n",
    "missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "lambda_dict_pct = {lam: (np.array(acc_dict[lam]) * 100) for lam in acc_dict}\n",
    "sorted_lambdas = sorted(lambda_dict_pct.keys())\n",
    "cmap = plt.cm.viridis\n",
    "colors = cmap(np.linspace(0, 1, len(sorted_lambdas)))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for idx, lam in enumerate(sorted_lambdas):\n",
    "    acc_pct = lambda_dict_pct[lam]\n",
    "    plt.plot(missing_rates_pct, acc_pct, linestyle='--', marker='o', markersize=4, linewidth=2.5, label=f\"λ = {lam}\",)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# x‐ticks\n",
    "plt.xticks(missing_rates_pct, fontsize=9)\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "for spine in ['left', 'bottom']:\n",
    "    ax.spines[spine].set_color('#888888')\n",
    "    ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# dynamically y‐axis: from min accuracy up to 100%\n",
    "all_acc_values = np.concatenate([lambda_dict_pct[lam] for lam in sorted_lambdas])\n",
    "y_min = all_acc_values.min()\n",
    "y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "ax.set_yticks(np.arange(y_min_tick, 101, 10))\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "ax.set_ylim(y_min, 100)\n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "# force legend to appear in top right\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.0085), frameon=True, edgecolor='#888888', facecolor='white', fontsize=8)\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "plt.savefig(f\"figures/accuracies/accs_latent_dim_{latent_dim}.pdf\")\n",
    "plt.savefig(f\"figures/accuracies/accs_latent_dim_{latent_dim}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2531577",
   "metadata": {},
   "source": [
    "### Plot the same classification accuracies but for a zoomed in plot for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert missing rates to percentages\n",
    "missing_rates_pct = [m * 100 for m in missing_rates]\n",
    "lambda_dict_pct = {lam: (np.array(acc_dict[lam]) * 100) for lam in acc_dict}\n",
    "sorted_lambdas = sorted(lambda_dict_pct.keys())\n",
    "cmap = plt.cm.viridis\n",
    "colors = cmap(np.linspace(0, 1, len(sorted_lambdas)))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for idx, lam in enumerate(sorted_lambdas):\n",
    "    acc_pct = lambda_dict_pct[lam]\n",
    "    plt.plot(missing_rates_pct, acc_pct, linestyle='--', marker='o', markersize=4, linewidth=1.0, label=f\"λ = {lam}\",)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "\n",
    "# x‐ticks\n",
    "x_thresh = 31\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0, x_thresh + 1, 10))\n",
    "ax.set_xlim(0, x_thresh)\n",
    "plt.xticks(fontsize=9)\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "for spine in ['left', 'bottom']:\n",
    "    ax.spines[spine].set_color('#888888')\n",
    "    ax.spines[spine].set_linewidth(0.8)\n",
    "\n",
    "# dynamically change y‐axis: from min accuracy up to 100%\n",
    "all_acc_values = np.concatenate([lambda_dict_pct[lam] for lam in sorted_lambdas])\n",
    "y_min = 89\n",
    "y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "ax.set_yticks(np.arange(y_min_tick, 101, 1))\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "ax.set_ylim(89.5, 97.5)\n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "# force legend to appear in top right\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.0085), frameon=True, edgecolor='#888888', facecolor='white', fontsize=8)\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "plt.savefig(f\"figures/accuracies/accs_latent_dim_{latent_dim}_zoomed.pdf\")\n",
    "plt.savefig(f\"figures/accuracies/accs_latent_dim_{latent_dim}_zoomed.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72939ad7",
   "metadata": {},
   "source": [
    "### Evaluate sample quality\n",
    "\n",
    "Plot a 3 by 3 grid of nine samples drawn from the model loaded most recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb02c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "n_bins = 32\n",
    "model.sampler.n_bins = n_bins\n",
    "z, log_w = model.sampler(seed=42)\n",
    "logits_tensor = model.decoder.net(z.to(device))\n",
    "chosen_idxs = sample(range(n_bins), 9)\n",
    "\n",
    "# 3 by 3 figure of samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "axes = axes.flatten()\n",
    "for ax, idx in zip(axes, chosen_idxs):\n",
    "    logits_sample = logits_tensor[idx]\n",
    "    probs = torch.sigmoid(logits_sample)\n",
    "    sample_flat = torch.bernoulli(probs)\n",
    "    img = sample_flat.view(28, 28).detach().cpu().numpy()\n",
    "\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"figures/samples/latent_dim_{latent_dim}/lam_{lamda:.1f}_latent_dim_{latent_dim}_samples.pdf\")\n",
    "# plt.savefig(f\"figures/samples/latent_dim_{latent_dim}/lam_{lamda:.1f}_latent_dim_{latent_dim}_samples.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206392ff",
   "metadata": {},
   "source": [
    "❗ fix up before meeting with Marco\n",
    "### Plots of the mean and standard deviations of our Monte Carlo estimators for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35624f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "\n",
    "# missing_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "# acc_dict = {k: [] for k in missing_rates}\n",
    "# num_runs = 50 # each run takes ~30s w/ N=2^9 and ~?s w/ N=2^14\n",
    "\n",
    "# for run in range(num_runs):\n",
    "#     rand_seed = randint(1, 2 ** 32 - 2)\n",
    "#     for missing_rate in missing_rates:\n",
    "#         n_bins_list, acc_list, test_lls_list = compute_accuracies(\n",
    "#             model=model,\n",
    "#             X_test=X_test,\n",
    "#             y_test=y_test,\n",
    "#             lower_power_bound=8,\n",
    "#             upper_power_bound=9,\n",
    "#             latent_opt=False,\n",
    "#             use_mnist=use_mnist,\n",
    "#             missing_rate=missing_rate,\n",
    "#             seed=rand_seed,\n",
    "#         )\n",
    "#         acc_dict[missing_rate].append(acc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # missing percentages\n",
    "# missing_rates = sorted(acc_dict)\n",
    "# missing_pct = [m * 100 for m in missing_rates]\n",
    "\n",
    "# # compute mean and std (%) for each missing rate\n",
    "# means_pct = np.array([np.mean(acc_dict[m]) * 100 for m in missing_rates])\n",
    "# stds_pct  = np.array([np.std(acc_dict[m], ddof=1) * 100 for m in missing_rates])\n",
    "# print(means_pct, stds_pct)\n",
    "\n",
    "# # upper and lower bounds\n",
    "# upper = means_pct + stds_pct\n",
    "# lower = means_pct - stds_pct\n",
    "\n",
    "# # just plotting things, not too important to understand\n",
    "# base_color = plt.cm.viridis(0.5)\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.fill_between(missing_pct, lower, upper, color=base_color, alpha=0.2) # shade region between the lower and upper lines\n",
    "# plt.plot(missing_pct, means_pct, color=base_color, linestyle='-', marker='o', markersize=4, linewidth=2.5, label='Mean accuracy') # mean line\n",
    "# plt.plot(missing_pct, upper, color=base_color, linestyle='--', linewidth=1) # upper line\n",
    "# plt.plot(missing_pct, lower, color=base_color, linestyle='--', linewidth=1) # lower line\n",
    "# plt.xlabel(\"Pixel values missing at random (%)\", fontsize=10)\n",
    "# plt.ylabel(\"Accuracy (%)\", fontsize=10)\n",
    "# plt.xticks(missing_pct, fontsize=9)\n",
    "# ax = plt.gca()\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# for spine in ['left', 'bottom']:\n",
    "#     ax.spines[spine].set_color('#888888')\n",
    "#     ax.spines[spine].set_linewidth(0.8)\n",
    "# y_min = lower.min()\n",
    "# y_min_tick = int(np.floor(y_min / 10) * 10)\n",
    "# ax.set_yticks(np.arange(y_min_tick, 101, 10))\n",
    "# ax.yaxis.grid(True, linestyle='--', linewidth=1.0, color='#bbbbbb')\n",
    "# ax.set_ylim(y_min, 100)\n",
    "# plt.yticks(fontsize=9)\n",
    "# plt.legend(loc='upper right', bbox_to_anchor=(1, 1.0085), frameon=True, edgecolor='#888888', facecolor='white', fontsize=8)\n",
    "# plt.tight_layout(pad=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891df11",
   "metadata": {},
   "source": [
    "# ❗ To be removed at submission ❗\n",
    "### Display just NLLs (this is to speed up obtaining them if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e782cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_bins_list = [2 ** k for k in range(8, 15)]\n",
    "# seed=42\n",
    "# seed_everything(seed=seed)\n",
    "\n",
    "# # model hyperparams\n",
    "# dataset = 'mnist' if use_mnist else 'bmnist'\n",
    "# latent_dim = 32\n",
    "# num_bins_trained = 8_192\n",
    "# version_num = 0\n",
    "# for lamda in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    \n",
    "#     # load model\n",
    "#     model_path = glob.glob(repo_dir+f'/logs/{dataset}/hybrid/latent_dim_{latent_dim}/num_bins_{num_bins_trained}/lambda_{lamda:.2f}/version_{version_num}/checkpoints/*.ckpt')[0]\n",
    "#     model = ContinuousMixture.load_from_checkpoint(model_path).to(device)\n",
    "#     model.n_chunks = 32\n",
    "#     model.missing = False # this was True before, check if difference (hope not)\n",
    "#     model.eval(); # semi-colon to prevent printing model architecture\n",
    "\n",
    "#     out = \"\"\n",
    "#     for n_bins in n_bins_list:\n",
    "#         model.sampler.n_bins = n_bins\n",
    "#         z, log_w = model.sampler(seed=seed)\n",
    "#         test_ll = -model.eval_loader(test_loader, z, log_w, device=device).mean().item()\n",
    "#         # print(f\"Mean NLL for {n_bins:5d} components: {test_ll:6.2f}\")\n",
    "#         out += f\"{test_ll:.2f} & \"\n",
    "#     print(\"\\hline\")\n",
    "#     print(f\"{lamda:.1f} & \" + out[:-3] + 2 * \"\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
